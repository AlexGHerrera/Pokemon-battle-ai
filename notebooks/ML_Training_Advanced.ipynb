{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631cfe6c",
   "metadata": {},
   "source": [
    "# ğŸ¤– Pokemon Battle AI - La BÃºsqueda del Modelo Definitivo\n",
    "\n",
    "En el mundo de las batallas Pokemon, cada decisiÃ³n cuenta. Cada movimiento, cada cambio, cada estrategia puede determinar la diferencia entre la victoria y la derrota. Nuestro viaje hasta ahora nos ha llevado desde el anÃ¡lisis exploratorio hasta un baseline sÃ³lido con **ROC-AUC de 0.837**.\n",
    "\n",
    "Pero sabemos que podemos hacer mejor. Mucho mejor.\n",
    "\n",
    "## ğŸ¯ La MisiÃ³n: Superar lo Imposible\n",
    "\n",
    "Hoy emprendemos la fase mÃ¡s emocionante de nuestro proyecto: **crear el modelo de Machine Learning mÃ¡s avanzado** para predecir batallas Pokemon. No nos conformamos con modelos simples; vamos a desplegar un arsenal completo de algoritmos de Ãºltima generaciÃ³n.\n",
    "\n",
    "### ğŸ—ºï¸ Nuestro Plan de Batalla\n",
    "\n",
    "Como entrenadores Pokemon experimentados, sabemos que la preparaciÃ³n es clave. Nuestro plan de entrenamiento seguirÃ¡ una estrategia meticulosa:\n",
    "\n",
    "**ğŸ”§ Fase 1: PreparaciÃ³n del Campo de Batalla**\n",
    "- Refinamiento de caracterÃ­sticas basado en insights del EDA\n",
    "- IngenierÃ­a de features que capturen la esencia de cada batalla\n",
    "- SelecciÃ³n inteligente de variables predictivas\n",
    "\n",
    "**âš”ï¸ Fase 2: Despliegue del Arsenal Base**\n",
    "- Logistic Regression: La elegancia de la simplicidad\n",
    "- Random Forest: El poder de la sabidurÃ­a colectiva\n",
    "- SVM: La precisiÃ³n matemÃ¡tica en acciÃ³n\n",
    "\n",
    "**ğŸš€ Fase 3: Armas de DestrucciÃ³n Masiva**\n",
    "- XGBoost: El campeÃ³n de Kaggle\n",
    "- LightGBM: Velocidad y precisiÃ³n combinadas\n",
    "- Neural Networks: La inteligencia artificial pura\n",
    "\n",
    "**âš™ï¸ Fase 4: Perfeccionamiento TÃ¡ctico**\n",
    "- Hyperparameter tuning con bÃºsqueda inteligente\n",
    "- Cross-validation para robustez mÃ¡xima\n",
    "- AnÃ¡lisis profundo de curvas de aprendizaje\n",
    "\n",
    "**ğŸ¤ Fase 5: La UniÃ³n Hace la Fuerza**\n",
    "- Ensemble de los mejores modelos\n",
    "- Voting strategies para decisiones consensuadas\n",
    "- Meta-learning para superar lÃ­mites individuales\n",
    "\n",
    "**ğŸ† Fase 6: El Momento de la Verdad**\n",
    "- EvaluaciÃ³n exhaustiva contra el baseline\n",
    "- AnÃ¡lisis de errores y casos lÃ­mite\n",
    "- SelecciÃ³n del modelo campeÃ³n\n",
    "\n",
    "Â¿Lograremos superar el **ROC-AUC de 0.837**? Â¿CuÃ¡nto podremos mejorar? El viaje comienza ahoraâ€¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de31bd9",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Armando Nuestro Arsenal: Las Herramientas del Maestro\n",
    "\n",
    "Como cualquier entrenador Pokemon sabe, tener las herramientas adecuadas es fundamental para el Ã©xito. En nuestro laboratorio de Machine Learning, cada librerÃ­a es como un Pokemon especializado, cada una con sus propias habilidades Ãºnicas.\n",
    "\n",
    "Vamos a importar nuestro equipo completo:\n",
    "- **Pandas & NumPy**: Nuestros Pikachu y Charizard, confiables y poderosos para manipulaciÃ³n de datos\n",
    "- **Scikit-learn**: El Mew de ML, versÃ¡til y con acceso a casi cualquier algoritmo\n",
    "- **XGBoost & LightGBM**: Los legendarios Rayquaza y Kyogre del gradient boosting\n",
    "- **Matplotlib & Seaborn**: Nuestros artistas Smeargle, creando visualizaciones que cuentan historias\n",
    "\n",
    "Cada importaciÃ³n nos acerca mÃ¡s a nuestro objetivo: crear el modelo mÃ¡s poderoso jamÃ¡s visto en batallas Pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fc333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LibrerÃ­as bÃ¡sicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    StratifiedKFold, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, \n",
    "    f1_score, classification_report, confusion_matrix, roc_curve, auc\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
    "\n",
    "# Modelos avanzados\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Utilidades\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Colores Pokemon\n",
    "POKEMON_COLORS = {\n",
    "    'fire': '#FF6B35',\n",
    "    'water': '#4A90E2', \n",
    "    'grass': '#7ED321',\n",
    "    'electric': '#F5A623',\n",
    "    'psychic': '#BD10E0',\n",
    "    'ice': '#50E3C2',\n",
    "    'dragon': '#9013FE',\n",
    "    'dark': '#4A4A4A',\n",
    "    'fighting': '#D0021B',\n",
    "    'poison': '#B8E986'\n",
    "}\n",
    "\n",
    "# Detectar entorno de ejecuciÃ³n\n",
    "import os\n",
    "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "WORKING_DIR = \"/kaggle/working\" if IS_KAGGLE else \".\"\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas correctamente\")\n",
    "print(f\"ğŸŒ Entorno detectado: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"ğŸ“ Directorio de trabajo: {WORKING_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46764944",
   "metadata": {},
   "source": [
    "## ğŸ“Š El Despertar de los Datos: Liberando el Arsenal Completo\n",
    "\n",
    "Cada dataset cuenta una historia, y el nuestro es **absolutamente Ã©pico**. Hemos pasado del entrenamiento con una muestra a desatar **TODO EL PODER** de nuestro arsenal de datos completo. Ya no son solo 2000 batallas - ahora tenemos acceso a **TODAS las batallas Pokemon disponibles**.\n",
    "\n",
    "Imagina por un momento: **Miles y miles de enfrentamientos Ãºnicos**, decenas de miles de decisiones crÃ­ticas, cientos de Pokemon diferentes luchando por la gloria en una escala nunca antes vista. Desde batallas rÃ¡pidas y decisivas hasta maratones Ã©picos, tenemos la biblioteca completa de la experiencia competitiva Pokemon.\n",
    "\n",
    "### ğŸ­ Los Protagonistas de Nuestra Historia\n",
    "\n",
    "Nuestros datos no son simples nÃºmeros; son las memorias digitales de entrenadores que:\n",
    "- Tomaron decisiones bajo presiÃ³n\n",
    "- Ejecutaron estrategias complejas\n",
    "- Experimentaron la emociÃ³n de la victoria y la amargura de la derrota\n",
    "\n",
    "Cada log de batalla es como un pergamino antiguo que debemos descifrar. Cada evento registrado - cada movimiento, cada cambio, cada momento crÃ­tico - contiene pistas sobre quÃ© hace que un entrenador triunfe sobre otro.\n",
    "\n",
    "**Â¿QuÃ© secretos revelarÃ¡n estos datos a escala masiva?** Con este arsenal completo de batallas, nuestros modelos tendrÃ¡n acceso a patrones que solo emergen con grandes volÃºmenes de datos. Â¿Descubriremos estrategias meta que solo son visibles con miles de batallas? Â¿Encontraremos correlaciones sutiles que se perdÃ­an en muestras mÃ¡s pequeÃ±as?\n",
    "\n",
    "**Â¡La aventura de entrenar con el dataset completo comienza ahora!** ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f41d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Cargar datos con estructura correcta (soluciÃ³n al problema de ganadores)\n",
    "try:\n",
    "    # Usar dataset pÃºblico de Kaggle: pokemon-showdown-battles-gen9-randbats (~14,000 batallas)\n",
    "    import glob\n",
    "    \n",
    "    # Usar dataset pÃºblico de Kaggle con estructura correcta (archivos en raÃ­z)\n",
    "    possible_patterns = [\n",
    "        \"/kaggle/input/pokemon-showdown-battles-gen9-randbats/*.json\",        # Dataset pÃºblico (raÃ­z) - CORRECTO\n",
    "        \"../data/battles/*.json\",                                             # Archivos locales para desarrollo\n",
    "        \"/kaggle/input/*/parsed/*.json\",                                      # Por si hay subcarpeta parsed/\n",
    "        \"/kaggle/input/*/*.json\",                                             # Fallback general\n",
    "    ]\n",
    "    \n",
    "    battle_files = []\n",
    "    for pattern in possible_patterns:\n",
    "        battle_files = glob.glob(pattern)\n",
    "        if battle_files:\n",
    "            print(f\"âœ… Encontrados archivos con patrÃ³n: {pattern}\")\n",
    "            break\n",
    "    \n",
    "    battles_data = []\n",
    "    print(f\"ğŸ” Encontrados {len(battle_files)} archivos de batalla\")\n",
    "    \n",
    "    for i, file_path in enumerate(battle_files):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                battle = json.load(f)\n",
    "                battles_data.append(battle)\n",
    "            \n",
    "            # Mostrar progreso cada 1000 archivos\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"ğŸ“Š Procesados {i + 1}/{len(battle_files)} archivos...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error procesando {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"âœ… Datos cargados desde archivos individuales: {len(battles_data)} batallas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error cargando desde Kaggle: {e}\")\n",
    "    # Fallback local para desarrollo\n",
    "    import glob\n",
    "    battle_files = glob.glob(\"../data/battles/*.json\")\n",
    "    \n",
    "    battles_data = []\n",
    "    print(f\"ğŸ” Encontrados {len(battle_files)} archivos locales\")\n",
    "    \n",
    "    for i, file_path in enumerate(battle_files):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                battle = json.load(f)\n",
    "                battles_data.append(battle)\n",
    "                \n",
    "            # Mostrar progreso cada 1000 archivos\n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"ğŸ“Š Procesados {i + 1}/{len(battle_files)} archivos...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error procesando {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"âœ… Dataset local cargado: {len(battles_data)} batallas\")\n",
    "    print(f\"ğŸš€ Â¡Usando archivos individuales con diversidad de ganadores garantizada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc483c",
   "metadata": {},
   "source": [
    "### ğŸ”§ La Alquimia de los Datos: Transformando Batallas en SabidurÃ­a\n",
    "\n",
    "Ahora viene la parte mÃ¡s artÃ­stica de nuestro proceso: la **ingenierÃ­a de caracterÃ­sticas**. Como un alquimista medieval transformando metales comunes en oro, vamos a convertir logs de batalla crudos en features predictivas poderosas.\n",
    "\n",
    "### ğŸ§¬ Decodificando el ADN de una Batalla\n",
    "\n",
    "Cada batalla Pokemon tiene su propio \"ADN\" - una secuencia Ãºnica de eventos que la define. Nuestro trabajo es extraer la esencia de este ADN y convertirla en nÃºmeros que nuestros algoritmos puedan entender.\n",
    "\n",
    "**Â¿QuÃ© hace que una batalla sea Ãºnica?**\n",
    "- **Intensidad**: Â¿Fue una batalla rÃ¡pida y brutal o un duelo prolongado de resistencia?\n",
    "- **Complejidad**: Â¿CuÃ¡ntos cambios estratÃ©gicos hubo? Â¿QuÃ© tan dinÃ¡mica fue?\n",
    "- **Agresividad**: Â¿Los entrenadores fueron directos o cautelosos?\n",
    "- **Adaptabilidad**: Â¿QuÃ© tan bien respondieron a las situaciones cambiantes?\n",
    "\n",
    "### ğŸ¯ Las MÃ©tricas que Importan\n",
    "\n",
    "BasÃ¡ndonos en nuestro anÃ¡lisis exploratorio previo, sabemos que ciertas mÃ©tricas son cruciales:\n",
    "- **Eventos de movimiento**: El corazÃ³n de cada batalla\n",
    "- **Ratios de daÃ±o**: La eficiencia ofensiva\n",
    "- **Patrones de cambio**: La flexibilidad tÃ¡ctica\n",
    "- **DuraciÃ³n e intensidad**: El ritmo de la batalla\n",
    "\n",
    "Cada feature que extraemos es como capturar la esencia de miles de decisiones estratÃ©gicas. Â¿Lograremos capturar los patrones que separan a los maestros de los novatos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fece9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_battle_features(battles_data):\n",
    "    \"\"\"\n",
    "    Extrae caracterÃ­sticas numÃ©ricas de las batallas para ML.\n",
    "    Basado en los hallazgos del EDA previo y estructura real de datos.\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    def get_in(data, keys, default=None):\n",
    "        \"\"\"FunciÃ³n auxiliar para acceso seguro a datos anidados.\"\"\"\n",
    "        for key in keys:\n",
    "            if isinstance(data, dict) and key in data:\n",
    "                data = data[key]\n",
    "            else:\n",
    "                return default\n",
    "        return data\n",
    "    \n",
    "    def calculate_battle_metrics(battle: dict) -> dict:\n",
    "        \"\"\"Calcula mÃ©tricas clave de una batalla (copiado del EDA).\"\"\"\n",
    "        metadata = battle.get('metadata', {})\n",
    "        turns = battle.get('turns', [])\n",
    "        \n",
    "        # MÃ©tricas bÃ¡sicas\n",
    "        total_turns = len(turns)\n",
    "        winner = get_in(metadata, ['outcome', 'winner'])\n",
    "        reason = get_in(metadata, ['outcome', 'reason'])\n",
    "        \n",
    "        # AnÃ¡lisis detallado de eventos\n",
    "        total_events = 0\n",
    "        move_events = 0\n",
    "        switch_events = 0\n",
    "        damage_events = 0\n",
    "        effect_events = 0\n",
    "        heal_events = 0\n",
    "        status_events = 0\n",
    "        \n",
    "        # MÃ©tricas de momentum y timing\n",
    "        early_game_events = 0  # Primeros 3 turnos\n",
    "        mid_game_events = 0    # Turnos 4-8\n",
    "        late_game_events = 0   # Turnos 9+\n",
    "        \n",
    "        # Patrones de decisiÃ³n\n",
    "        consecutive_moves = 0\n",
    "        consecutive_switches = 0\n",
    "        last_action_type = None\n",
    "        current_streak = 0\n",
    "        \n",
    "        for turn_idx, turn in enumerate(turns):\n",
    "            turn_events = turn.get('events', [])\n",
    "            turn_event_count = len(turn_events)\n",
    "            total_events += turn_event_count\n",
    "            \n",
    "            # Clasificar eventos por fase del juego\n",
    "            if turn_idx < 3:\n",
    "                early_game_events += turn_event_count\n",
    "            elif turn_idx < 8:\n",
    "                mid_game_events += turn_event_count\n",
    "            else:\n",
    "                late_game_events += turn_event_count\n",
    "            \n",
    "            # Analizar tipos de eventos\n",
    "            for event in turn_events:\n",
    "                event_type = event.get('type', '').lower()\n",
    "                \n",
    "                if event_type == 'move':\n",
    "                    move_events += 1\n",
    "                elif event_type == 'switch':\n",
    "                    switch_events += 1\n",
    "                elif 'damage' in event_type:\n",
    "                    damage_events += 1\n",
    "                elif 'heal' in event_type:\n",
    "                    heal_events += 1\n",
    "                elif 'faint' in event_type:\n",
    "                    status_events += 1\n",
    "                else:\n",
    "                    effect_events += 1\n",
    "                \n",
    "                # Rastrear patrones consecutivos\n",
    "                if event_type in ['move', 'switch']:\n",
    "                    if event_type == last_action_type:\n",
    "                        current_streak += 1\n",
    "                    else:\n",
    "                        if last_action_type == 'move':\n",
    "                            consecutive_moves = max(consecutive_moves, current_streak)\n",
    "                        elif last_action_type == 'switch':\n",
    "                            consecutive_switches = max(consecutive_switches, current_streak)\n",
    "                        current_streak = 1\n",
    "                        last_action_type = event_type\n",
    "        \n",
    "        # Finalizar rastreo de patrones\n",
    "        if last_action_type == 'move':\n",
    "            consecutive_moves = max(consecutive_moves, current_streak)\n",
    "        elif last_action_type == 'switch':\n",
    "            consecutive_switches = max(consecutive_switches, current_streak)\n",
    "        \n",
    "        return {\n",
    "            'total_turns': total_turns,\n",
    "            'winner': winner,\n",
    "            'reason': reason,\n",
    "            'total_events': total_events,\n",
    "            'move_events': move_events,\n",
    "            'switch_events': switch_events,\n",
    "            'damage_events': damage_events,\n",
    "            'heal_events': heal_events,\n",
    "            'status_events': status_events,\n",
    "            'effect_events': effect_events,\n",
    "            'events_per_turn': total_events / max(total_turns, 1),\n",
    "            'early_game_intensity': early_game_events / max(min(total_turns, 3), 1),\n",
    "            'mid_game_intensity': mid_game_events / max(min(total_turns - 3, 5), 1) if total_turns > 3 else 0,\n",
    "            'late_game_intensity': late_game_events / max(total_turns - 8, 1) if total_turns > 8 else 0,\n",
    "            'move_switch_ratio': move_events / max(switch_events, 1),\n",
    "            'consecutive_moves': consecutive_moves,\n",
    "            'consecutive_switches': consecutive_switches,\n",
    "            'action_diversity': len(set([e.get('type') for turn in turns for e in turn.get('events', [])]))\n",
    "        }\n",
    "    \n",
    "    def extract_team_composition_features(battle: dict) -> dict:\n",
    "        \"\"\"Extrae features de composiciÃ³n de equipos (copiado del EDA).\"\"\"\n",
    "        teams = get_in(battle, [\"team_revelation\", \"teams\"], {})\n",
    "        features = {}\n",
    "        \n",
    "        for player_id in ['p1', 'p2']:\n",
    "            team = teams.get(player_id, [])\n",
    "            if isinstance(team, list) and team:\n",
    "                # MÃ©tricas bÃ¡sicas del equipo\n",
    "                levels = [p.get('level', 0) for p in team if p.get('level')]\n",
    "                hps = [get_in(p, ['base_stats', 'hp']) for p in team if get_in(p, ['base_stats', 'hp'])]\n",
    "                \n",
    "                # EstadÃ­sticas de nivel\n",
    "                avg_level = np.mean(levels) if levels else 0\n",
    "                level_std = np.std(levels) if len(levels) > 1 else 0\n",
    "                \n",
    "                # EstadÃ­sticas de HP\n",
    "                avg_hp = np.mean(hps) if hps else 0\n",
    "                hp_std = np.std(hps) if len(hps) > 1 else 0\n",
    "                total_hp = sum(hps) if hps else 0\n",
    "                \n",
    "                # Diversidad y revelaciÃ³n\n",
    "                species_count = len(set(p.get('species') for p in team if p.get('species')))\n",
    "                fully_revealed = sum(1 for p in team if p.get('revelation_status') == 'fully_revealed')\n",
    "                partially_revealed = sum(1 for p in team if p.get('revelation_status') == 'partially_revealed')\n",
    "                \n",
    "                # InformaciÃ³n conocida\n",
    "                known_abilities = sum(1 for p in team if p.get('known_ability'))\n",
    "                known_items = sum(1 for p in team if p.get('known_item'))\n",
    "                total_known_moves = sum(len(p.get('known_moves', [])) for p in team)\n",
    "                \n",
    "                features.update({\n",
    "                    f'{player_id}_team_size': len(team),\n",
    "                    f'{player_id}_avg_level': avg_level,\n",
    "                    f'{player_id}_level_std': level_std,\n",
    "                    f'{player_id}_min_level': min(levels) if levels else 0,\n",
    "                    f'{player_id}_max_level': max(levels) if levels else 0,\n",
    "                    f'{player_id}_avg_hp': avg_hp,\n",
    "                    f'{player_id}_hp_std': hp_std,\n",
    "                    f'{player_id}_total_hp': total_hp,\n",
    "                    f'{player_id}_species_diversity': species_count,\n",
    "                    f'{player_id}_fully_revealed': fully_revealed,\n",
    "                    f'{player_id}_partially_revealed': partially_revealed,\n",
    "                    f'{player_id}_known_abilities': known_abilities,\n",
    "                    f'{player_id}_known_items': known_items,\n",
    "                    f'{player_id}_total_known_moves': total_known_moves,\n",
    "                    f'{player_id}_info_advantage': fully_revealed + partially_revealed * 0.5\n",
    "                })\n",
    "            else:\n",
    "                # Valores por defecto si no hay datos del equipo\n",
    "                for metric in ['team_size', 'avg_level', 'level_std', 'min_level', 'max_level', \n",
    "                              'avg_hp', 'hp_std', 'total_hp', 'species_diversity', \n",
    "                              'fully_revealed', 'partially_revealed', 'known_abilities', \n",
    "                              'known_items', 'total_known_moves', 'info_advantage']:\n",
    "                    features[f'{player_id}_{metric}'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # Procesar cada batalla\n",
    "    for i, battle in enumerate(battles_data):\n",
    "        try:\n",
    "            # MÃ©tricas bÃ¡sicas mejoradas\n",
    "            metrics = calculate_battle_metrics(battle)\n",
    "            \n",
    "            # Features de composiciÃ³n de equipos\n",
    "            team_features = extract_team_composition_features(battle)\n",
    "            \n",
    "            # Combinar todas las features\n",
    "            features = {\n",
    "                'battle_id': battle.get('battle_id'),\n",
    "                'format': battle.get('format_id', ''),\n",
    "                'turns': metrics['total_turns'],\n",
    "                'winner': metrics['winner'],\n",
    "                'reason': metrics['reason'],\n",
    "                'total_events': metrics['total_events'],\n",
    "                'move_events': metrics['move_events'],\n",
    "                'switch_events': metrics['switch_events'],\n",
    "                'damage_events': metrics['damage_events'],\n",
    "                'heal_events': metrics['heal_events'],\n",
    "                'status_events': metrics['status_events'],\n",
    "                'effect_events': metrics['effect_events'],\n",
    "                'events_per_turn': metrics['events_per_turn'],\n",
    "                'early_game_intensity': metrics['early_game_intensity'],\n",
    "                'mid_game_intensity': metrics['mid_game_intensity'],\n",
    "                'late_game_intensity': metrics['late_game_intensity'],\n",
    "                'move_switch_ratio': metrics['move_switch_ratio'],\n",
    "                'consecutive_moves': metrics['consecutive_moves'],\n",
    "                'consecutive_switches': metrics['consecutive_switches'],\n",
    "                'action_diversity': metrics['action_diversity']\n",
    "            }\n",
    "            \n",
    "            # InformaciÃ³n de jugadores\n",
    "            players = battle.get('players', {})\n",
    "            for player_id in ['p1', 'p2']:\n",
    "                player_info = players.get(player_id, {})\n",
    "                features[f'{player_id}_rating'] = player_info.get('ladder_rating_pre', 0)\n",
    "            \n",
    "            # Agregar features de composiciÃ³n de equipos\n",
    "            features.update(team_features)\n",
    "            \n",
    "            # Features de ventaja competitiva\n",
    "            if features['p1_rating'] and features['p2_rating']:\n",
    "                features['rating_difference'] = abs(features['p1_rating'] - features['p2_rating'])\n",
    "                features['rating_advantage_p1'] = features['p1_rating'] - features['p2_rating']\n",
    "            else:\n",
    "                features['rating_difference'] = 0\n",
    "                features['rating_advantage_p1'] = 0\n",
    "            \n",
    "            # Features de balance de equipos\n",
    "            features['team_size_difference'] = abs(features['p1_team_size'] - features['p2_team_size'])\n",
    "            features['level_advantage_p1'] = features['p1_avg_level'] - features['p2_avg_level']\n",
    "            features['hp_advantage_p1'] = features['p1_total_hp'] - features['p2_total_hp']\n",
    "            features['info_advantage_p1'] = features['p1_info_advantage'] - features['p2_info_advantage']\n",
    "            \n",
    "            # Ratios importantes (corregidos)\n",
    "            if features['total_events'] > 0:\n",
    "                features['switch_ratio'] = features['switch_events'] / features['total_events']\n",
    "                features['move_ratio'] = features['move_events'] / features['total_events']\n",
    "                features['damage_ratio'] = features['damage_events'] / features['total_events']\n",
    "            else:\n",
    "                features['switch_ratio'] = 0\n",
    "                features['move_ratio'] = 0\n",
    "                features['damage_ratio'] = 0\n",
    "            \n",
    "            # MÃ©tricas de intensidad\n",
    "            features['battle_intensity'] = (features['damage_events'] + features['status_events']) / max(features['turns'], 1)\n",
    "            \n",
    "            features_list.append(features)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando batalla {battle.get('battle_id', 'unknown')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "\n",
    "# Extraer caracterÃ­sticas\n",
    "print(\"ğŸ”§ Extrayendo caracterÃ­sticas de las batallas...\")\n",
    "df_features = extract_battle_features(battles_data)\n",
    "\n",
    "# Codificar variables categÃ³ricas\n",
    "label_encoders = {}\n",
    "categorical_cols = ['format', 'winner']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_features.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_features[f'{col}_encoded'] = le.fit_transform(df_features[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"âœ… CaracterÃ­sticas extraÃ­das: {df_features.shape}\")\n",
    "print(f\"ğŸ“Š Columnas: {list(df_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0bd02",
   "metadata": {},
   "source": [
    "### ğŸ“ˆ El Primer Vistazo: Â¿QuÃ© Nos Susurran los Datos?\n",
    "\n",
    "Antes de lanzarnos a entrenar modelos complejos, necesitamos escuchar lo que nuestros datos tienen que decirnos. Como un entrenador Pokemon experimentado que observa el campo antes de la batalla, vamos a hacer un reconocimiento rÃ¡pido pero crucial.\n",
    "\n",
    "### ğŸ² El Equilibrio del Universo Pokemon\n",
    "\n",
    "Una pregunta fundamental: **Â¿Nuestros datos estÃ¡n balanceados?** En el mundo real de las batallas Pokemon, Â¿hay un sesgo hacia algÃºn tipo de ganador? Â¿O vivimos en un universo perfectamente equilibrado donde la habilidad es el Ãºnico factor determinante?\n",
    "\n",
    "### ğŸ” Los Primeros Indicios del Ã‰xito\n",
    "\n",
    "TambiÃ©n vamos a echar un vistazo a las correlaciones iniciales. Â¿QuÃ© caracterÃ­sticas muestran las primeras seÃ±ales de ser predictivas? Es como observar las primeras cartas en una partida de poker - no nos dice todo, pero nos da pistas valiosas sobre quÃ© esperar.\n",
    "\n",
    "**Â¿QuÃ© patrones emergerÃ¡n?** Â¿ConfirmarÃ¡n nuestras hipÃ³tesis del EDA o nos sorprenderÃ¡n con revelaciones inesperadas? Los nÃºmeros estÃ¡n a punto de hablarâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar distribuciÃ³n del target\n",
    "if 'winner_encoded' in df_features.columns:\n",
    "    # Debug: Verificar valores Ãºnicos del winner original\n",
    "    print(\"Valores Ãºnicos de 'winner':\", df_features['winner'].unique())\n",
    "    print(\"Conteo de valores de 'winner':\")\n",
    "    print(df_features['winner'].value_counts())\n",
    "    \n",
    "    # Verificar si hay al menos 2 clases diferentes\n",
    "    unique_winners = df_features['winner'].nunique()\n",
    "    if unique_winners < 2:\n",
    "        print(f\"âš ï¸  ADVERTENCIA: Solo hay {unique_winners} clase(s) Ãºnica(s) en 'winner'\")\n",
    "        print(\"Esto causarÃ¡ errores en el entrenamiento de ML\")\n",
    "        print(\"Verificando datos de batalla...\")\n",
    "        \n",
    "        # Mostrar algunas batallas de ejemplo para debug\n",
    "        print(\"\\nEjemplos de datos de batalla:\")\n",
    "        for i, battle in enumerate(battles_data[:5]):\n",
    "            print(f\"Batalla {i+1}: winner = {battle.get('winner', 'N/A')}\")\n",
    "    \n",
    "    winner_dist = df_features['winner_encoded'].value_counts()\n",
    "    print(\"DistribuciÃ³n del ganador:\")\n",
    "    print(winner_dist)\n",
    "    \n",
    "    # Visualizar distribuciÃ³n\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    winner_dist.plot(kind='bar', color=[POKEMON_COLORS['fire'], POKEMON_COLORS['water']], alpha=0.8)\n",
    "    plt.title('DistribuciÃ³n de Ganadores', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Ganador Codificado')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Correlaciones importantes\n",
    "    numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Remover el target de las correlaciones para evitar correlaciÃ³n perfecta consigo mismo\n",
    "    feature_cols = [col for col in numeric_cols if col not in ['winner_encoded']]\n",
    "    \n",
    "    if len(feature_cols) > 0 and 'winner_encoded' in df_features.columns:\n",
    "        # Calcular correlaciones solo con las caracterÃ­sticas, no con el target\n",
    "        corr_matrix = df_features[feature_cols + ['winner_encoded']].corr()\n",
    "        corr_with_target = corr_matrix['winner_encoded'].abs().drop('winner_encoded').sort_values(ascending=False)\n",
    "        top_corr = corr_with_target.head(8)\n",
    "        \n",
    "        top_corr.plot(kind='barh', color=POKEMON_COLORS['electric'], alpha=0.8)\n",
    "        plt.title('Top Correlaciones con Ganador', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('CorrelaciÃ³n Absoluta')\n",
    "        \n",
    "        # Debug: Mostrar las correlaciones mÃ¡s altas\n",
    "        print(f\"\\nğŸ” TOP CORRELACIONES CON GANADOR:\")\n",
    "        print(\"-\" * 50)\n",
    "        for feature, corr in top_corr.head(5).items():\n",
    "            print(f\"   {feature:20} | CorrelaciÃ³n: {corr:.4f}\")\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No hay datos suficientes\\npara correlaciones', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar grÃ¡fico para documentaciÃ³n tÃ©cnica\n",
    "    if IS_KAGGLE:\n",
    "        plots_dir = Path(f\"{WORKING_DIR}/plots\")\n",
    "    else:\n",
    "        plots_dir = Path(\"../plots\")\n",
    "    \n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(plots_dir / \"00_data_distribution_analysis.png\", \n",
    "                dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    print(f\"ğŸ’¾ GrÃ¡fico guardado: {plots_dir / '00_data_distribution_analysis.png'}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518d2f9",
   "metadata": {},
   "source": [
    "## ğŸ¤– El Laboratorio del Dr. Frankenstein: Creando Nuestros Monstruos de ML\n",
    "\n",
    "Ha llegado el momento mÃ¡s emocionante: **crear nuestros modelos de Machine Learning**. Como el Dr. Frankenstein en su laboratorio, vamos a dar vida a siete criaturas diferentes, cada una con sus propias fortalezas, debilidades y personalidades Ãºnicas.\n",
    "\n",
    "### ğŸ§ª La Clase PokemonMLTrainer: Nuestro Laboratorio Personal\n",
    "\n",
    "Hemos diseÃ±ado una clase especial que actuarÃ¡ como nuestro laboratorio de experimentaciÃ³n. Esta no es una clase ordinaria; es un **centro de comando avanzado** que:\n",
    "\n",
    "- **Gestiona mÃºltiples experimentos simultÃ¡neamente**\n",
    "- **EvalÃºa el rendimiento con mÃ©tricas sofisticadas**\n",
    "- **Genera visualizaciones que cuentan historias**\n",
    "- **Optimiza automÃ¡ticamente los hiperparÃ¡metros**\n",
    "- **Crea ensembles inteligentes**\n",
    "- **Analiza errores como un detective**\n",
    "\n",
    "### ğŸ­ Conoce a Nuestros Siete Gladiadores\n",
    "\n",
    "Cada modelo que vamos a entrenar tiene su propia \"personalidad\" y enfoque para resolver el problema:\n",
    "\n",
    "**ğŸ¯ Logistic Regression**: El estratega clÃ¡sico, elegante y directo\n",
    "**ğŸŒ³ Random Forest**: El consejo de ancianos, sabidurÃ­a colectiva\n",
    "**âš¡ Gradient Boosting**: El perfeccionista, aprende de cada error\n",
    "**ğŸš€ XGBoost**: El campeÃ³n de competencias, optimizado para ganar\n",
    "**ğŸ’¨ LightGBM**: El velocista inteligente, rÃ¡pido pero preciso\n",
    "**ğŸ§  Neural Network**: El cerebro artificial, patrones complejos\n",
    "**âš”ï¸ SVM**: El matemÃ¡tico puro, fronteras de decisiÃ³n perfectas\n",
    "\n",
    "**Â¿CuÃ¡l de estos gladiadores se alzarÃ¡ como campeÃ³n?** Â¿O serÃ¡ que la verdadera magia ocurre cuando trabajen juntos? El torneo estÃ¡ a punto de comenzarâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06dc54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Importar librerÃ­as adicionales para anÃ¡lisis avanzado\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "from scipy import stats\n",
    "\n",
    "class PokemonMLTrainer:\n",
    "    \"\"\"Entrenador avanzado de Machine Learning para batallas Pokemon.\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42, save_plots=True, plots_dir=None):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.best_score = 0.0\n",
    "        self.scaler = StandardScaler()\n",
    "        self.learning_curves = {}\n",
    "        self.roc_curves = {}\n",
    "        self.save_plots = save_plots\n",
    "        \n",
    "        # Detectar entorno y configurar rutas apropiadas\n",
    "        if IS_KAGGLE:\n",
    "            # Entorno Kaggle\n",
    "            self.plots_dir = Path(f\"{WORKING_DIR}/plots\")\n",
    "        elif plots_dir is not None:\n",
    "            # Ruta personalizada\n",
    "            self.plots_dir = Path(plots_dir)\n",
    "        else:\n",
    "            # Desarrollo local\n",
    "            self.plots_dir = Path(\"../plots\")\n",
    "        \n",
    "        # Crear directorio de plots si no existe\n",
    "        if self.save_plots:\n",
    "            self.plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"ğŸ“ Directorio de grÃ¡ficos creado: {self.plots_dir}\")\n",
    "    \n",
    "    def save_plot(self, filename, dpi=300, bbox_inches='tight'):\n",
    "        \"\"\"Guarda el plot actual con alta calidad para documentaciÃ³n tÃ©cnica.\"\"\"\n",
    "        if self.save_plots:\n",
    "            filepath = self.plots_dir / f\"{filename}.png\"\n",
    "            plt.savefig(filepath, dpi=dpi, bbox_inches=bbox_inches, \n",
    "                       facecolor='white', edgecolor='none')\n",
    "            print(f\"ğŸ’¾ GrÃ¡fico guardado: {filepath}\")\n",
    "        return filepath if self.save_plots else None\n",
    "    \n",
    "    def generate_plots_index(self):\n",
    "        \"\"\"Genera un Ã­ndice de todos los grÃ¡ficos exportados para documentaciÃ³n tÃ©cnica.\"\"\"\n",
    "        if not self.save_plots:\n",
    "            return\n",
    "            \n",
    "        plots_info = [\n",
    "            (\"00_data_distribution_analysis.png\", \"DistribuciÃ³n de Datos y Correlaciones\", \"AnÃ¡lisis inicial de la distribuciÃ³n de ganadores y correlaciones con el target\"),\n",
    "            (\"01_roc_curves_comparison.png\", \"Curvas ROC - ComparaciÃ³n de Modelos\", \"ComparaciÃ³n del rendimiento de todos los modelos usando curvas ROC\"),\n",
    "            (\"02_precision_recall_curves.png\", \"Curvas Precision-Recall\", \"AnÃ¡lisis de precisiÃ³n y recall para cada modelo\"),\n",
    "            (\"03_calibration_curves.png\", \"Curvas de CalibraciÃ³n\", \"EvaluaciÃ³n de la confiabilidad de las probabilidades predichas\"),\n",
    "            (\"04_feature_importance_analysis.png\", \"AnÃ¡lisis de Importancia de CaracterÃ­sticas\", \"Consenso entre modelos sobre las caracterÃ­sticas mÃ¡s importantes\"),\n",
    "            (\"05_prediction_errors_analysis.png\", \"AnÃ¡lisis de Errores de PredicciÃ³n\", \"InvestigaciÃ³n detallada de los patrones de error del mejor modelo\")\n",
    "        ]\n",
    "        \n",
    "        index_path = self.plots_dir / \"README_PLOTS.md\"\n",
    "        with open(index_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# ğŸ“Š Ãndice de GrÃ¡ficos - Pokemon Battle AI\\n\\n\")\n",
    "            f.write(\"Este directorio contiene todos los grÃ¡ficos generados durante el entrenamiento del modelo de ML para batallas Pokemon.\\n\\n\")\n",
    "            f.write(\"## ğŸ“ˆ GrÃ¡ficos Disponibles\\n\\n\")\n",
    "            \n",
    "            for filename, title, description in plots_info:\n",
    "                f.write(f\"### {title}\\n\")\n",
    "                f.write(f\"**Archivo:** `{filename}`\\n\\n\")\n",
    "                f.write(f\"**DescripciÃ³n:** {description}\\n\\n\")\n",
    "                f.write(f\"![{title}]({filename})\\n\\n\")\n",
    "                f.write(\"---\\n\\n\")\n",
    "            \n",
    "            f.write(\"## ğŸ¯ Uso en DocumentaciÃ³n TÃ©cnica\\n\\n\")\n",
    "            f.write(\"Todos los grÃ¡ficos estÃ¡n guardados en alta resoluciÃ³n (300 DPI) con fondo blanco, \")\n",
    "            f.write(\"optimizados para su inclusiÃ³n en documentos tÃ©cnicos, presentaciones y reportes.\\n\\n\")\n",
    "            f.write(\"**Formato:** PNG con transparencia\\n\")\n",
    "            f.write(\"**ResoluciÃ³n:** 300 DPI\\n\")\n",
    "            f.write(\"**Colores:** Paleta Pokemon temÃ¡tica\\n\")\n",
    "        \n",
    "        print(f\"ğŸ“‹ Ãndice de grÃ¡ficos creado: {index_path}\")\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Configura todos los modelos a entrenar.\"\"\"\n",
    "        models = {\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                random_state=self.random_state,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            \n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            \n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            \n",
    "            'xgboost': xgb.XGBClassifier(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                random_state=self.random_state,\n",
    "                eval_metric='logloss',\n",
    "                use_label_encoder=False\n",
    "            ),\n",
    "            \n",
    "            'lightgbm': lgb.LGBMClassifier(\n",
    "                n_estimators=200,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=6,\n",
    "                random_state=self.random_state,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            \n",
    "            'neural_network': MLPClassifier(\n",
    "                hidden_layer_sizes=(256, 128, 64),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                learning_rate_init=0.001,\n",
    "                max_iter=500,\n",
    "                random_state=self.random_state,\n",
    "                early_stopping=True,\n",
    "                validation_fraction=0.1\n",
    "            ),\n",
    "            \n",
    "            'svm': SVC(\n",
    "                kernel='rbf',\n",
    "                probability=True,\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def prepare_data(self, df, target_col='winner_encoded', test_size=0.2):\n",
    "        \"\"\"Prepara los datos para entrenamiento.\"\"\"\n",
    "        # Seleccionar caracterÃ­sticas numÃ©ricas\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        if target_col in numeric_cols:\n",
    "            numeric_cols.remove(target_col)\n",
    "        \n",
    "        # Remover columnas no Ãºtiles\n",
    "        exclude_cols = ['battle_id']\n",
    "        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "        \n",
    "        X = df[numeric_cols].copy()\n",
    "        y = df[target_col].copy()\n",
    "        \n",
    "        # Manejar valores faltantes\n",
    "        X = X.fillna(X.median())\n",
    "        \n",
    "        # DivisiÃ³n de datos\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, numeric_cols\n",
    "    \n",
    "    def calculate_advanced_metrics(self, y_true, y_pred, y_pred_proba, model_name):\n",
    "        \"\"\"Calcula mÃ©tricas avanzadas especÃ­ficas por tipo de modelo.\"\"\"\n",
    "        \n",
    "        # MÃ©tricas bÃ¡sicas\n",
    "        basic_metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "            'f1': f1_score(y_true, y_pred, average='weighted'),\n",
    "            'roc_auc': roc_auc_score(y_true, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # MÃ©tricas avanzadas\n",
    "        advanced_metrics = {}\n",
    "        \n",
    "        # Brier Score (calibraciÃ³n de probabilidades)\n",
    "        from sklearn.metrics import brier_score_loss\n",
    "        advanced_metrics['brier_score'] = brier_score_loss(y_true, y_pred_proba)\n",
    "        \n",
    "        # Log Loss\n",
    "        from sklearn.metrics import log_loss\n",
    "        try:\n",
    "            advanced_metrics['log_loss'] = log_loss(y_true, y_pred_proba)\n",
    "        except:\n",
    "            advanced_metrics['log_loss'] = np.nan\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        from sklearn.metrics import matthews_corrcoef\n",
    "        advanced_metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
    "        \n",
    "        # Balanced Accuracy\n",
    "        from sklearn.metrics import balanced_accuracy_score\n",
    "        advanced_metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # MÃ©tricas especÃ­ficas por clase\n",
    "        precision_per_class = precision_score(y_true, y_pred, average=None)\n",
    "        recall_per_class = recall_score(y_true, y_pred, average=None)\n",
    "        \n",
    "        advanced_metrics['precision_class_0'] = precision_per_class[0] if len(precision_per_class) > 0 else 0\n",
    "        advanced_metrics['precision_class_1'] = precision_per_class[1] if len(precision_per_class) > 1 else 0\n",
    "        advanced_metrics['recall_class_0'] = recall_per_class[0] if len(recall_per_class) > 0 else 0\n",
    "        advanced_metrics['recall_class_1'] = recall_per_class[1] if len(recall_per_class) > 1 else 0\n",
    "        \n",
    "        # Combinar mÃ©tricas\n",
    "        all_metrics = {**basic_metrics, **advanced_metrics}\n",
    "        \n",
    "        return all_metrics\n",
    "    \n",
    "    def plot_learning_curves(self, model, X, y, model_name, cv=5):\n",
    "        \"\"\"Genera curvas de aprendizaje para un modelo.\"\"\"\n",
    "        \n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, cv=cv, n_jobs=-1, \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            scoring='roc_auc', random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        # Calcular medias y desviaciones estÃ¡ndar\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        # Guardar para anÃ¡lisis posterior\n",
    "        self.learning_curves[model_name] = {\n",
    "            'train_sizes': train_sizes,\n",
    "            'train_mean': train_mean,\n",
    "            'train_std': train_std,\n",
    "            'val_mean': val_mean,\n",
    "            'val_std': val_std\n",
    "        }\n",
    "        \n",
    "        return train_sizes, train_mean, train_std, val_mean, val_std\n",
    "    \n",
    "    def plot_roc_curves(self, models_results, X_test, y_test):\n",
    "        \"\"\"Genera curvas ROC para todos los modelos.\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = [POKEMON_COLORS['fire'], POKEMON_COLORS['water'], \n",
    "                 POKEMON_COLORS['grass'], POKEMON_COLORS['electric'],\n",
    "                 POKEMON_COLORS['psychic'], POKEMON_COLORS['ice'],\n",
    "                 POKEMON_COLORS['dragon']]\n",
    "        \n",
    "        for i, (model_name, result) in enumerate(models_results.items()):\n",
    "            if i >= len(colors):\n",
    "                break\n",
    "                \n",
    "            y_pred_proba = result['probabilities']\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Guardar curva ROC\n",
    "            self.roc_curves[model_name] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
    "            \n",
    "            plt.plot(fpr, tpr, color=colors[i], lw=2, alpha=0.8,\n",
    "                    label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "        \n",
    "        # LÃ­nea diagonal (clasificador aleatorio)\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.8)\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Tasa de Falsos Positivos', fontsize=12)\n",
    "        plt.ylabel('Tasa de Verdaderos Positivos', fontsize=12)\n",
    "        plt.title('Curvas ROC - ComparaciÃ³n de Modelos', fontsize=16, fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # AÃ±adir lÃ­nea de baseline\n",
    "        baseline_auc = 0.837\n",
    "        plt.axhline(y=baseline_auc, color='red', linestyle=':', alpha=0.7, \n",
    "                   label=f'Baseline AUC = {baseline_auc}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico\n",
    "        self.save_plot(\"01_roc_curves_comparison\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_precision_recall_curves(self, models_results, X_test, y_test):\n",
    "        \"\"\"Genera curvas Precision-Recall para todos los modelos.\"\"\"\n",
    "        \n",
    "        from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = [POKEMON_COLORS['fire'], POKEMON_COLORS['water'], \n",
    "                 POKEMON_COLORS['grass'], POKEMON_COLORS['electric'],\n",
    "                 POKEMON_COLORS['psychic'], POKEMON_COLORS['ice'],\n",
    "                 POKEMON_COLORS['dragon']]\n",
    "        \n",
    "        for i, (model_name, result) in enumerate(models_results.items()):\n",
    "            if i >= len(colors):\n",
    "                break\n",
    "                \n",
    "            y_pred_proba = result['probabilities']\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "            avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "            \n",
    "            plt.plot(recall, precision, color=colors[i], lw=2, alpha=0.8,\n",
    "                    label=f'{model_name} (AP = {avg_precision:.3f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Curvas Precision-Recall - ComparaciÃ³n de Modelos', fontsize=16, fontweight='bold')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico\n",
    "        self.save_plot(\"02_precision_recall_curves\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_calibration_curves(self, models_results, X_test, y_test):\n",
    "        \"\"\"Genera curvas de calibraciÃ³n para evaluar la confiabilidad de las probabilidades.\"\"\"\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = [POKEMON_COLORS['fire'], POKEMON_COLORS['water'], \n",
    "                 POKEMON_COLORS['grass'], POKEMON_COLORS['electric'],\n",
    "                 POKEMON_COLORS['psychic'], POKEMON_COLORS['ice'],\n",
    "                 POKEMON_COLORS['dragon']]\n",
    "        \n",
    "        for i, (model_name, result) in enumerate(models_results.items()):\n",
    "            if i >= len(colors):\n",
    "                break\n",
    "                \n",
    "            y_pred_proba = result['probabilities']\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_test, y_pred_proba, n_bins=10\n",
    "            )\n",
    "            \n",
    "            plt.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                    color=colors[i], alpha=0.8, label=f'{model_name}')\n",
    "        \n",
    "        # LÃ­nea de calibraciÃ³n perfecta\n",
    "        plt.plot([0, 1], [0, 1], \"k:\", label=\"CalibraciÃ³n perfecta\")\n",
    "        \n",
    "        plt.xlabel('Probabilidad Predicha Promedio', fontsize=12)\n",
    "        plt.ylabel('FracciÃ³n de Positivos', fontsize=12)\n",
    "        plt.title('Curvas de CalibraciÃ³n - Confiabilidad de Probabilidades', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico\n",
    "        self.save_plot(\"03_calibration_curves\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_importance_analysis(self, models_results, feature_names):\n",
    "        \"\"\"Analiza y visualiza la importancia de caracterÃ­sticas con consenso entre modelos.\"\"\"\n",
    "        \n",
    "        # Recopilar importancias de todos los modelos\n",
    "        feature_importances = {}\n",
    "        \n",
    "        for model_name, result in models_results.items():\n",
    "            model = result['model']\n",
    "            \n",
    "            # Obtener importancias segÃºn el tipo de modelo\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                # Tree-based models\n",
    "                importances = model.feature_importances_\n",
    "            elif hasattr(model, 'coef_'):\n",
    "                # Linear models\n",
    "                importances = np.abs(model.coef_[0])\n",
    "            else:\n",
    "                # Para otros modelos, usar permutation importance\n",
    "                continue\n",
    "            \n",
    "            feature_importances[model_name] = importances\n",
    "        \n",
    "        if not feature_importances:\n",
    "            print(\"âš ï¸ No se pudieron extraer importancias de caracterÃ­sticas\")\n",
    "            return None\n",
    "        \n",
    "        # Crear DataFrame de importancias\n",
    "        importance_df = pd.DataFrame(feature_importances, index=feature_names)\n",
    "        \n",
    "        # Calcular estadÃ­sticas de consenso\n",
    "        importance_df['mean'] = importance_df.mean(axis=1)\n",
    "        importance_df['std'] = importance_df.std(axis=1)\n",
    "        importance_df['stability'] = 1 - (importance_df['std'] / (importance_df['mean'] + 1e-8))\n",
    "        \n",
    "        # Ordenar por importancia promedio\n",
    "        importance_df = importance_df.sort_values('mean', ascending=False)\n",
    "        \n",
    "        # VisualizaciÃ³n\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Top 10 caracterÃ­sticas por importancia promedio\n",
    "        top_features = importance_df.head(10)\n",
    "        axes[0, 0].barh(range(len(top_features)), top_features['mean'], \n",
    "                       color=POKEMON_COLORS['fire'], alpha=0.8)\n",
    "        axes[0, 0].set_yticks(range(len(top_features)))\n",
    "        axes[0, 0].set_yticklabels(top_features.index)\n",
    "        axes[0, 0].set_title('Top 10 CaracterÃ­sticas - Importancia Promedio', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Importancia Promedio')\n",
    "        \n",
    "        # 2. Estabilidad vs Importancia\n",
    "        axes[0, 1].scatter(importance_df['mean'], importance_df['stability'], \n",
    "                          c=POKEMON_COLORS['water'], alpha=0.7, s=60)\n",
    "        axes[0, 1].set_xlabel('Importancia Promedio')\n",
    "        axes[0, 1].set_ylabel('Estabilidad (1 - CV)')\n",
    "        axes[0, 1].set_title('Estabilidad vs Importancia', fontweight='bold')\n",
    "        \n",
    "        # Anotar caracterÃ­sticas mÃ¡s estables e importantes\n",
    "        stable_important = importance_df[(importance_df['stability'] > 0.7) & \n",
    "                                       (importance_df['mean'] > importance_df['mean'].median())]\n",
    "        for idx, row in stable_important.head(5).iterrows():\n",
    "            axes[0, 1].annotate(idx, (row['mean'], row['stability']), \n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        # 3. Heatmap de importancias por modelo\n",
    "        models_to_plot = list(feature_importances.keys())[:5]  # Top 5 modelos\n",
    "        features_to_plot = top_features.index[:8]  # Top 8 caracterÃ­sticas\n",
    "        \n",
    "        heatmap_data = importance_df.loc[features_to_plot, models_to_plot]\n",
    "        im = axes[1, 0].imshow(heatmap_data.values, cmap='YlOrRd', aspect='auto')\n",
    "        axes[1, 0].set_xticks(range(len(models_to_plot)))\n",
    "        axes[1, 0].set_xticklabels(models_to_plot, rotation=45)\n",
    "        axes[1, 0].set_yticks(range(len(features_to_plot)))\n",
    "        axes[1, 0].set_yticklabels(features_to_plot)\n",
    "        axes[1, 0].set_title('Heatmap de Importancias por Modelo', fontweight='bold')\n",
    "        \n",
    "        # AÃ±adir colorbar\n",
    "        plt.colorbar(im, ax=axes[1, 0])\n",
    "        \n",
    "        # 4. DistribuciÃ³n de importancias\n",
    "        axes[1, 1].hist(importance_df['mean'], bins=20, color=POKEMON_COLORS['grass'], \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].axvline(importance_df['mean'].mean(), color='red', linestyle='--', \n",
    "                          label=f'Media: {importance_df[\"mean\"].mean():.4f}')\n",
    "        axes[1, 1].set_xlabel('Importancia Promedio')\n",
    "        axes[1, 1].set_ylabel('Frecuencia')\n",
    "        axes[1, 1].set_title('DistribuciÃ³n de Importancias', fontweight='bold')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico\n",
    "        self.save_plot(\"04_feature_importance_analysis\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Reporte de caracterÃ­sticas mÃ¡s importantes\n",
    "        print(\"\\nğŸ† TOP 10 CARACTERÃSTICAS MÃS IMPORTANTES:\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, (feature, row) in enumerate(top_features.iterrows(), 1):\n",
    "            stability_emoji = \"ğŸ”’\" if row['stability'] > 0.8 else \"ğŸ“Š\" if row['stability'] > 0.6 else \"ğŸ“ˆ\"\n",
    "            print(f\"{i:2d}. {stability_emoji} {feature:25} | Importancia: {row['mean']:.4f} | Estabilidad: {row['stability']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ CARACTERÃSTICAS MÃS ESTABLES (Estabilidad > 0.7): {len(importance_df[importance_df['stability'] > 0.7])}\")\n",
    "        print(f\"âš¡ CARACTERÃSTICAS ALTAMENTE PREDICTIVAS (Importancia > media): {len(importance_df[importance_df['mean'] > importance_df['mean'].mean()])}\")\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def hyperparameter_optimization(self, X_train, y_train, top_models=3):\n",
    "        \"\"\"Optimiza hiperparÃ¡metros para los mejores modelos.\"\"\"\n",
    "        \n",
    "        print(f\"\\nâš™ï¸ Optimizando hiperparÃ¡metros para los top {top_models} modelos...\")\n",
    "        \n",
    "        # Obtener los mejores modelos por ROC-AUC\n",
    "        model_scores = [(name, result['metrics']['roc_auc']) for name, result in self.results.items()]\n",
    "        model_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_model_names = [name for name, _ in model_scores[:top_models]]\n",
    "        \n",
    "        optimized_models = {}\n",
    "        \n",
    "        # Definir espacios de bÃºsqueda para cada modelo\n",
    "        param_grids = {\n",
    "            'random_forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'num_leaves': [31, 50, 100],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'gradient_boosting': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'subsample': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'logistic_regression': {\n",
    "                'C': [0.1, 1.0, 10.0, 100.0],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for model_name in top_model_names:\n",
    "            if model_name not in param_grids:\n",
    "                print(f\"âš ï¸ No hay parÃ¡metros definidos para {model_name}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nğŸ”§ Optimizando {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Obtener modelo base\n",
    "                base_model = self.setup_models()[model_name]\n",
    "                \n",
    "                # Preparar datos\n",
    "                if model_name in ['neural_network', 'svm', 'logistic_regression']:\n",
    "                    X_train_final = self.scaler.fit_transform(X_train)\n",
    "                else:\n",
    "                    X_train_final = X_train\n",
    "                \n",
    "                # BÃºsqueda aleatoria\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    base_model,\n",
    "                    param_grids[model_name],\n",
    "                    n_iter=20,  # Reducido para velocidad\n",
    "                    cv=3,       # Reducido para velocidad\n",
    "                    scoring='roc_auc',\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                random_search.fit(X_train_final, y_train)\n",
    "                \n",
    "                optimized_models[f\"{model_name}_optimized\"] = {\n",
    "                    'model': random_search.best_estimator_,\n",
    "                    'best_params': random_search.best_params_,\n",
    "                    'best_score': random_search.best_score_,\n",
    "                    'cv_results': random_search.cv_results_\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ… {model_name} optimizado - CV Score: {random_search.best_score_:.4f}\")\n",
    "                print(f\"   Mejores parÃ¡metros: {random_search.best_params_}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error optimizando {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return optimized_models\n",
    "    \n",
    "    def create_ensemble(self, models_results, X_train, y_train):\n",
    "        \"\"\"Crea un modelo ensemble con los mejores modelos.\"\"\"\n",
    "        \n",
    "        print(\"\\nğŸ¤ Creando modelo ensemble...\")\n",
    "        \n",
    "        try:\n",
    "            # Seleccionar los mejores modelos (top 5)\n",
    "            model_scores = [(name, result['metrics']['roc_auc']) for name, result in models_results.items()]\n",
    "            model_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_models = model_scores[:5]\n",
    "            \n",
    "            print(f\"   Modelos seleccionados para ensemble:\")\n",
    "            for name, score in top_models:\n",
    "                print(f\"   - {name}: {score:.4f}\")\n",
    "            \n",
    "            # Crear lista de estimadores para el ensemble\n",
    "            estimators = []\n",
    "            for name, _ in top_models:\n",
    "                model = models_results[name]['model']\n",
    "                estimators.append((name, model))\n",
    "            \n",
    "            # Crear VotingClassifier con soft voting\n",
    "            ensemble = VotingClassifier(\n",
    "                estimators=estimators,\n",
    "                voting='soft'\n",
    "            )\n",
    "            \n",
    "            # Entrenar ensemble\n",
    "            ensemble.fit(X_train, y_train)\n",
    "            \n",
    "            print(\"âœ… Modelo ensemble creado exitosamente\")\n",
    "            return ensemble\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error creando ensemble: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_prediction_errors(self, models_results, X_test, y_test, feature_names):\n",
    "        \"\"\"Analiza los errores de predicciÃ³n para entender patrones de fallo.\"\"\"\n",
    "        \n",
    "        print(\"\\nğŸ” Analizando errores de predicciÃ³n...\")\n",
    "        \n",
    "        # Obtener el mejor modelo\n",
    "        best_model_name = max(models_results.keys(), \n",
    "                             key=lambda x: models_results[x]['metrics']['roc_auc'])\n",
    "        best_result = models_results[best_model_name]\n",
    "        \n",
    "        y_pred = best_result['predictions']\n",
    "        y_pred_proba = best_result['probabilities']\n",
    "        \n",
    "        # Crear DataFrame de anÃ¡lisis\n",
    "        error_df = pd.DataFrame({\n",
    "            'true_label': y_test,\n",
    "            'predicted_label': y_pred,\n",
    "            'predicted_proba': y_pred_proba,\n",
    "            'correct': y_test == y_pred\n",
    "        })\n",
    "        \n",
    "        # AÃ±adir caracterÃ­sticas\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            error_df[feature] = X_test.iloc[:, i].values\n",
    "        \n",
    "        # Calcular mÃ©tricas de error\n",
    "        error_df['prediction_confidence'] = np.abs(error_df['predicted_proba'] - 0.5)\n",
    "        error_df['error_type'] = 'Correct'\n",
    "        error_df.loc[(error_df['true_label'] == 1) & (error_df['predicted_label'] == 0), 'error_type'] = 'False Negative'\n",
    "        error_df.loc[(error_df['true_label'] == 0) & (error_df['predicted_label'] == 1), 'error_type'] = 'False Positive'\n",
    "        \n",
    "        # Visualizaciones\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. DistribuciÃ³n de confianza por tipo de error\n",
    "        error_types = error_df['error_type'].unique()\n",
    "        colors = [POKEMON_COLORS['grass'], POKEMON_COLORS['fire'], POKEMON_COLORS['water']]\n",
    "        \n",
    "        for i, error_type in enumerate(error_types):\n",
    "            if i < len(colors):\n",
    "                subset = error_df[error_df['error_type'] == error_type]\n",
    "                axes[0, 0].hist(subset['prediction_confidence'], alpha=0.7, \n",
    "                              label=error_type, color=colors[i], bins=20)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Confianza de PredicciÃ³n')\n",
    "        axes[0, 0].set_ylabel('Frecuencia')\n",
    "        axes[0, 0].set_title('DistribuciÃ³n de Confianza por Tipo de Error', fontweight='bold')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # 2. Probabilidades predichas vs reales\n",
    "        axes[0, 1].scatter(error_df[error_df['correct']]['predicted_proba'], \n",
    "                          error_df[error_df['correct']]['true_label'], \n",
    "                          alpha=0.6, color=POKEMON_COLORS['grass'], label='Correctas', s=30)\n",
    "        axes[0, 1].scatter(error_df[~error_df['correct']]['predicted_proba'], \n",
    "                          error_df[~error_df['correct']]['true_label'], \n",
    "                          alpha=0.8, color=POKEMON_COLORS['fire'], label='Incorrectas', s=30)\n",
    "        axes[0, 1].set_xlabel('Probabilidad Predicha')\n",
    "        axes[0, 1].set_ylabel('Etiqueta Real')\n",
    "        axes[0, 1].set_title('Probabilidades Predichas vs Etiquetas Reales', fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # 3. Matriz de confusiÃ³n\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        im = axes[1, 0].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "        axes[1, 0].set_title('Matriz de ConfusiÃ³n', fontweight='bold')\n",
    "        \n",
    "        # AÃ±adir texto a la matriz\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                axes[1, 0].text(j, i, format(cm[i, j], 'd'),\n",
    "                               ha=\"center\", va=\"center\",\n",
    "                               color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        axes[1, 0].set_ylabel('Etiqueta Real')\n",
    "        axes[1, 0].set_xlabel('Etiqueta Predicha')\n",
    "        \n",
    "        # 4. CaracterÃ­sticas mÃ¡s diferentes en errores\n",
    "        incorrect_cases = error_df[~error_df['correct']]\n",
    "        correct_cases = error_df[error_df['correct']]\n",
    "        \n",
    "        feature_diffs = []\n",
    "        for feature in feature_names[:10]:  # Top 10 caracterÃ­sticas\n",
    "            if feature in error_df.columns:\n",
    "                diff = abs(incorrect_cases[feature].mean() - correct_cases[feature].mean())\n",
    "                feature_diffs.append((feature, diff))\n",
    "        \n",
    "        feature_diffs.sort(key=lambda x: x[1], reverse=True)\n",
    "        features, diffs = zip(*feature_diffs[:8])\n",
    "        \n",
    "        axes[1, 1].barh(range(len(features)), diffs, color=POKEMON_COLORS['psychic'], alpha=0.8)\n",
    "        axes[1, 1].set_yticks(range(len(features)))\n",
    "        axes[1, 1].set_yticklabels(features)\n",
    "        axes[1, 1].set_xlabel('Diferencia Promedio')\n",
    "        axes[1, 1].set_title('CaracterÃ­sticas MÃ¡s Diferentes en Errores', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar grÃ¡fico\n",
    "        self.save_plot(\"05_prediction_errors_analysis\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # EstadÃ­sticas de error\n",
    "        print(f\"\\nğŸ“Š ESTADÃSTICAS DE ERROR ({best_model_name}):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total de predicciones: {len(error_df)}\")\n",
    "        print(f\"Predicciones correctas: {error_df['correct'].sum()} ({error_df['correct'].mean()*100:.1f}%)\")\n",
    "        print(f\"Falsos positivos: {len(error_df[error_df['error_type'] == 'False Positive'])}\")\n",
    "        print(f\"Falsos negativos: {len(error_df[error_df['error_type'] == 'False Negative'])}\")\n",
    "        \n",
    "        # Casos de baja confianza\n",
    "        low_confidence = error_df[error_df['prediction_confidence'] < 0.1]\n",
    "        print(f\"\\nCasos de baja confianza (< 0.1): {len(low_confidence)} ({len(low_confidence)/len(error_df)*100:.1f}%)\")\n",
    "        print(f\"PrecisiÃ³n en casos de baja confianza: {low_confidence['correct'].mean()*100:.1f}%\")\n",
    "        \n",
    "        return error_df\n",
    "    \n",
    "    def train_models(self, X_train, X_test, y_train, y_test, feature_names):\n",
    "        \"\"\"Entrena todos los modelos con anÃ¡lisis avanzado.\"\"\"\n",
    "        print(\"ğŸš€ Iniciando entrenamiento de modelos con anÃ¡lisis avanzado...\")\n",
    "        \n",
    "        # Escalado para modelos que lo necesitan\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        models = self.setup_models()\n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nğŸ”„ Entrenando {model_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Seleccionar datos apropiados\n",
    "                if model_name in ['neural_network', 'svm', 'logistic_regression']:\n",
    "                    X_train_final = X_train_scaled\n",
    "                    X_test_final = X_test_scaled\n",
    "                else:\n",
    "                    X_train_final = X_train\n",
    "                    X_test_final = X_test\n",
    "                \n",
    "                # Entrenar modelo\n",
    "                model.fit(X_train_final, y_train)\n",
    "                \n",
    "                # Predicciones\n",
    "                y_pred = model.predict(X_test_final)\n",
    "                y_pred_proba = model.predict_proba(X_test_final)[:, 1]\n",
    "                \n",
    "                # MÃ©tricas avanzadas\n",
    "                metrics = self.calculate_advanced_metrics(y_test, y_pred, y_pred_proba, model_name)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(\n",
    "                    model, X_train_final, y_train, \n",
    "                    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
    "                    scoring='roc_auc'\n",
    "                )\n",
    "                \n",
    "                metrics['cv_mean'] = cv_scores.mean()\n",
    "                metrics['cv_std'] = cv_scores.std()\n",
    "                \n",
    "                # Curvas de aprendizaje (solo para modelos rÃ¡pidos)\n",
    "                if model_name in ['logistic_regression', 'random_forest', 'gradient_boosting']:\n",
    "                    print(f\"   ğŸ“ˆ Generando curvas de aprendizaje para {model_name}...\")\n",
    "                    self.plot_learning_curves(model, X_train_final, y_train, model_name)\n",
    "                \n",
    "                results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'metrics': metrics,\n",
    "                    'predictions': y_pred,\n",
    "                    'probabilities': y_pred_proba\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ… {model_name} - ROC-AUC: {metrics['roc_auc']:.4f} | MCC: {metrics['mcc']:.4f} | Brier: {metrics['brier_score']:.4f}\")\n",
    "                \n",
    "                # Actualizar mejor modelo\n",
    "                if metrics['roc_auc'] > self.best_score:\n",
    "                    self.best_score = metrics['roc_auc']\n",
    "                    self.best_model = model_name\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error entrenando {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        self.models = {name: result['model'] for name, result in results.items()}\n",
    "        self.results = results\n",
    "        \n",
    "        # Generar visualizaciones comparativas\n",
    "        print(\"\\nğŸ“Š Generando visualizaciones comparativas...\")\n",
    "        self.plot_roc_curves(results, X_test, y_test)\n",
    "        self.plot_precision_recall_curves(results, X_test, y_test)\n",
    "        self.plot_calibration_curves(results, X_test, y_test)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95705c6",
   "metadata": {},
   "source": [
    "## ğŸš€ Â¡Que Comience el EspectÃ¡culo! El Gran Torneo de Algoritmos\n",
    "\n",
    "**Ladies and gentlemen, trainers and Pokemon masters!** Ha llegado el momento que todos estÃ¡bamos esperando. DespuÃ©s de semanas de preparaciÃ³n, anÃ¡lisis y diseÃ±o, nuestros siete gladiadores estÃ¡n listos para enfrentarse en la arena mÃ¡s desafiante: **predecir el resultado de batallas Pokemon reales**.\n",
    "\n",
    "### ğŸª El Escenario EstÃ¡ Preparado\n",
    "\n",
    "Nuestros datos estÃ¡n pulidos y listos. Nuestras caracterÃ­sticas han sido cuidadosamente seleccionadas y engineered. Nuestros modelos estÃ¡n configurados con parÃ¡metros iniciales inteligentes. Todo estÃ¡ en su lugar para el espectÃ¡culo del siglo.\n",
    "\n",
    "### âš¡ La TensiÃ³n en el Aire\n",
    "\n",
    "Podemos sentir la electricidad en el ambiente. Cada algoritmo \"sabe\" que estÃ¡ compitiendo no solo contra los datos, sino contra otros seis competidores igualmente determinados. El baseline de **ROC-AUC 0.837** se alza como el dragÃ³n final que todos deben derrotar.\n",
    "\n",
    "**Â¿QuiÃ©n serÃ¡ el primero en caer?** Â¿QuÃ© modelo sorprenderÃ¡ con un rendimiento inesperado? Â¿Veremos una batalla reÃ±ida o habrÃ¡ un claro dominador desde el principio?\n",
    "\n",
    "La preparaciÃ³n ha terminado. Los dados estÃ¡n echados. **Â¡Que comience la batalla!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar entrenador\n",
    "trainer = PokemonMLTrainer(random_state=42)\n",
    "print(\"âœ… Entrenador ML avanzado inicializado\")\n",
    "print(\"ğŸš€ Â¡Listo para comenzar el entrenamiento!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "print(\"ğŸ“Š Preparando datos para entrenamiento...\")\n",
    "X_train, X_test, y_train, y_test, feature_names = trainer.prepare_data(df_features)\n",
    "\n",
    "print(f\"âœ… Datos preparados:\")\n",
    "print(f\"   - Entrenamiento: {X_train.shape}\")\n",
    "print(f\"   - Prueba: {X_test.shape}\")\n",
    "print(f\"   - CaracterÃ­sticas: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b4da6",
   "metadata": {},
   "source": [
    "### ğŸ¤– Round 1: Los Gladiadores Entran en AcciÃ³n\n",
    "\n",
    "**Â¡DING DING DING!** Suena la campana y nuestros siete gladiadores saltan al ring. Este es el momento de la verdad - despuÃ©s de toda la preparaciÃ³n, finalmente veremos de quÃ© estÃ¡n hechos nuestros modelos.\n",
    "\n",
    "### ğŸ­ El Drama se Desarrolla\n",
    "\n",
    "Cada modelo aborda el problema desde su perspectiva Ãºnica:\n",
    "- **Logistic Regression** entra con confianza clÃ¡sica, buscando relaciones lineales claras\n",
    "- **Random Forest** despliega su ejÃ©rcito de Ã¡rboles, cada uno votando por su predicciÃ³n favorita\n",
    "- **Gradient Boosting** comienza lentamente, aprendiendo meticulosamente de cada error\n",
    "- **XGBoost** llega con toda la experiencia de miles de competencias de Kaggle\n",
    "- **LightGBM** se mueve con agilidad felina, optimizando cada cÃ¡lculo\n",
    "- **Neural Network** activa sus neuronas, buscando patrones que otros no pueden ver\n",
    "- **SVM** traza fronteras de decisiÃ³n con precisiÃ³n matemÃ¡tica\n",
    "\n",
    "### ğŸ“Š Las MÃ©tricas Que Importan\n",
    "\n",
    "No nos conformamos con una sola mÃ©trica. Como jueces experimentados, evaluamos cada modelo desde mÃºltiples Ã¡ngulos:\n",
    "- **ROC-AUC**: Â¿QuÃ© tan bien separa ganadores de perdedores?\n",
    "- **MCC**: Â¿QuÃ© tan balanceado es su rendimiento?\n",
    "- **Brier Score**: Â¿QuÃ© tan calibradas estÃ¡n sus probabilidades?\n",
    "- **Cross-validation**: Â¿Es consistente o solo tuvo suerte?\n",
    "\n",
    "**Â¿QuiÃ©n tomarÃ¡ la delantera inicial?** Los primeros resultados estÃ¡n a punto de revelarseâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelos\n",
    "results = trainer.train_models(X_train, X_test, y_train, y_test, feature_names)\n",
    "\n",
    "print(f\"\\nğŸ† Mejor modelo base: {trainer.best_model} (ROC-AUC: {trainer.best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182a435",
   "metadata": {},
   "source": [
    "### ğŸ” Los Secretos Revelados: Â¿QuÃ© Hace Ganar una Batalla?\n",
    "\n",
    "Ahora que nuestros modelos han mostrado sus cartas, es momento de la **gran revelaciÃ³n**. Como detectives investigando un misterio, vamos a descubrir quÃ© caracterÃ­sticas son realmente importantes para predecir el Ã©xito en las batallas Pokemon.\n",
    "\n",
    "### ğŸ•µï¸ La InvestigaciÃ³n Comienza\n",
    "\n",
    "No todos los modelos \"ven\" las mismas cosas. Algunos se enfocan en patrones sutiles, otros en seÃ±ales obvias. Pero cuando mÃºltiples modelos coinciden en que una caracterÃ­stica es importante, sabemos que hemos encontrado algo especial.\n",
    "\n",
    "### ğŸ¯ El Consenso de los Maestros\n",
    "\n",
    "Vamos a realizar un anÃ¡lisis de consenso - como reunir a los mejores entrenadores Pokemon del mundo y preguntarles: **\"Â¿En quÃ© se fijan cuando predicen quiÃ©n ganarÃ¡ una batalla?\"**\n",
    "\n",
    "**Â¿SerÃ¡ la intensidad de la batalla?** Â¿La cantidad de movimientos ejecutados? Â¿Los patrones de cambio? Â¿O descubriremos algo completamente inesperado?\n",
    "\n",
    "### ğŸ† Las CaracterÃ­sticas MÃ¡s Estables\n",
    "\n",
    "TambiÃ©n buscaremos las caracterÃ­sticas mÃ¡s \"estables\" - aquellas en las que todos los modelos confÃ­an consistentemente. Estas son como las reglas fundamentales del universo Pokemon, principios que trascienden algoritmos especÃ­ficos.\n",
    "\n",
    "**Â¿QuÃ© secretos del Ã©xito en batallas Pokemon estÃ¡n a punto de ser revelados?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83074ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de importancia de caracterÃ­sticas\n",
    "print(\"\\nğŸ” Analizando importancia de caracterÃ­sticas...\")\n",
    "importance_analysis = trainer.plot_feature_importance_analysis(results, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96050ce",
   "metadata": {},
   "source": [
    "### âš™ï¸ Round 2: Los Gladiadores Evolucionan\n",
    "\n",
    "**Â¡Plot twist!** Como en cualquier buena historia de Pokemon, nuestros competidores estÃ¡n a punto de **evolucionar**. Los tres mejores modelos del primer round han ganado el derecho a una transformaciÃ³n especial: optimizaciÃ³n de hiperparÃ¡metros.\n",
    "\n",
    "### ğŸ§¬ La EvoluciÃ³n No Es Solo Suerte\n",
    "\n",
    "En el mundo Pokemon, la evoluciÃ³n requiere las condiciones exactas. De manera similar, nuestros modelos necesitan los hiperparÃ¡metros perfectos para alcanzar su mÃ¡ximo potencial. No es magia - es **ciencia pura y bÃºsqueda inteligente**.\n",
    "\n",
    "### ğŸ¯ La BÃºsqueda del Santo Grial\n",
    "\n",
    "Cada modelo tiene su propio \"cÃ³digo genÃ©tico\" de hiperparÃ¡metros:\n",
    "- **Random Forest**: Â¿CuÃ¡ntos Ã¡rboles? Â¿QuÃ© profundidad? Â¿CuÃ¡ntas muestras por hoja?\n",
    "- **XGBoost**: Â¿QuÃ© learning rate? Â¿CuÃ¡nta regularizaciÃ³n? Â¿QuÃ© subsample?\n",
    "- **LightGBM**: Â¿CuÃ¡ntas hojas? Â¿QuÃ© profundidad mÃ¡xima? Â¿CuÃ¡ntos estimadores?\n",
    "\n",
    "### ğŸ”¬ RandomizedSearchCV: Nuestro Laboratorio de EvoluciÃ³n\n",
    "\n",
    "No vamos a probar cada combinaciÃ³n posible (eso tomarÃ­a aÃ±os). En su lugar, usamos **bÃºsqueda aleatoria inteligente** - como un entrenador Pokemon experimentado que sabe exactamente quÃ© condiciones probar para cada evoluciÃ³n.\n",
    "\n",
    "**Â¿Veremos mejoras dramÃ¡ticas?** Â¿AlgÃºn modelo darÃ¡ un salto cuÃ¡ntico en rendimiento? Â¿O descubriremos que ya estaban cerca de su potencial mÃ¡ximo?\n",
    "\n",
    "**Â¡La evoluciÃ³n comienza ahora!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f936648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OptimizaciÃ³n de hiperparÃ¡metros\n",
    "optimized_models = trainer.hyperparameter_optimization(X_train, y_train, top_models=3)\n",
    "\n",
    "# Evaluar modelos optimizados\n",
    "print(\"\\nğŸ“Š Evaluando modelos optimizados...\")\n",
    "optimized_results = {}\n",
    "\n",
    "for opt_name, opt_data in optimized_models.items():\n",
    "    model = opt_data['model']\n",
    "    \n",
    "    # Seleccionar datos apropiados\n",
    "    base_name = opt_name.replace('_optimized', '')\n",
    "    if base_name in ['neural_network', 'svm', 'logistic_regression']:\n",
    "        X_test_final = trainer.scaler.transform(X_test)\n",
    "    else:\n",
    "        X_test_final = X_test\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_final)\n",
    "    y_pred_proba = model.predict_proba(X_test_final)[:, 1]\n",
    "    \n",
    "    # MÃ©tricas\n",
    "    metrics = trainer.calculate_advanced_metrics(y_test, y_pred, y_pred_proba, opt_name)\n",
    "    \n",
    "    optimized_results[opt_name] = {\n",
    "        'model': model,\n",
    "        'metrics': metrics,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… {opt_name} - ROC-AUC: {metrics['roc_auc']:.4f} | MCC: {metrics['mcc']:.4f}\")\n",
    "    \n",
    "    # Actualizar mejor modelo si es necesario\n",
    "    if metrics['roc_auc'] > trainer.best_score:\n",
    "        trainer.best_score = metrics['roc_auc']\n",
    "        trainer.best_model = opt_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41f9627",
   "metadata": {},
   "source": [
    "### ğŸ¤ La Alianza Definitiva: Cuando los Rivales se Unen\n",
    "\n",
    "En las mejores historias Ã©picas, llega un momento cuando los antiguos rivales deben unir fuerzas para enfrentar un desafÃ­o mayor. **Este es ese momento.**\n",
    "\n",
    "### ğŸŒŸ El Poder de la UniÃ³n\n",
    "\n",
    "Hemos visto lo que cada modelo puede lograr individualmente. Algunos brillan en ciertos aspectos, otros dominan diferentes patrones. Pero Â¿quÃ© pasarÃ­a si combinÃ¡ramos sus fortalezas Ãºnicas en una **sÃºper-alianza**?\n",
    "\n",
    "### ğŸ­ Los Avengers del Machine Learning\n",
    "\n",
    "Como los Avengers, cada modelo aporta algo Ãºnico al equipo:\n",
    "- **Random Forest**: La sabidurÃ­a colectiva y estabilidad\n",
    "- **XGBoost**: La precisiÃ³n competitiva y optimizaciÃ³n\n",
    "- **LightGBM**: La velocidad y eficiencia\n",
    "- **Neural Network**: La capacidad de ver patrones complejos\n",
    "- **Gradient Boosting**: El aprendizaje meticuloso de errores\n",
    "\n",
    "### ğŸ—³ï¸ Democracia en AcciÃ³n: Soft Voting\n",
    "\n",
    "Nuestro ensemble no es una dictadura donde un modelo domina. Es una **democracia perfecta** donde cada modelo vota con sus probabilidades, y la decisiÃ³n final emerge del consenso colectivo.\n",
    "\n",
    "### ğŸ¯ La Pregunta del MillÃ³n\n",
    "\n",
    "**Â¿SerÃ¡ el ensemble superior a cualquier modelo individual?** En teorÃ­a, deberÃ­a ser asÃ­ - la diversidad de enfoques deberÃ­a crear un predictor mÃ¡s robusto y preciso.\n",
    "\n",
    "Pero la teorÃ­a y la realidad a veces divergen. **Â¿Nuestros gladiadores trabajarÃ¡n mejor juntos o en solitario?**\n",
    "\n",
    "**Â¡La alianza definitiva estÃ¡ a punto de formarse!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d28064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ensemble\n",
    "ensemble_model = trainer.create_ensemble(results, X_train, y_train)\n",
    "\n",
    "if ensemble_model is not None:\n",
    "    # Evaluar ensemble\n",
    "    print(\"\\nğŸ“Š Evaluando modelo ensemble...\")\n",
    "    \n",
    "    # Predicciones del ensemble\n",
    "    y_pred_ensemble = ensemble_model.predict(X_test)\n",
    "    y_pred_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # MÃ©tricas del ensemble\n",
    "    ensemble_metrics = trainer.calculate_advanced_metrics(\n",
    "        y_test, y_pred_ensemble, y_pred_proba_ensemble, 'ensemble'\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ¤ Ensemble - ROC-AUC: {ensemble_metrics['roc_auc']:.4f} | MCC: {ensemble_metrics['mcc']:.4f}\")\n",
    "    \n",
    "    # Actualizar mejor modelo si el ensemble es superior\n",
    "    if ensemble_metrics['roc_auc'] > trainer.best_score:\n",
    "        trainer.best_score = ensemble_metrics['roc_auc']\n",
    "        trainer.best_model = 'ensemble'\n",
    "        \n",
    "        # AÃ±adir ensemble a resultados\n",
    "        optimized_results['ensemble'] = {\n",
    "            'model': ensemble_model,\n",
    "            'metrics': ensemble_metrics,\n",
    "            'predictions': y_pred_ensemble,\n",
    "            'probabilities': y_pred_proba_ensemble\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e173c7",
   "metadata": {},
   "source": [
    "### ğŸ” CSI: Pokemon Battle Edition - Investigando los Misterios del Fracaso\n",
    "\n",
    "Incluso los mejores entrenadores Pokemon pierden batallas. Incluso los mejores modelos de ML cometen errores. Pero la diferencia entre un buen entrenador y un **maestro** estÃ¡ en cÃ³mo aprende de esas derrotas.\n",
    "\n",
    "### ğŸ•µï¸ ConvirtiÃ©ndonos en Detectives de Datos\n",
    "\n",
    "Cada predicciÃ³n incorrecta es como una escena del crimen que debemos investigar. **Â¿Por quÃ© fallÃ³ nuestro modelo?** Â¿Fue mala suerte, informaciÃ³n insuficiente, o hay patrones sistemÃ¡ticos en nuestros errores?\n",
    "\n",
    "### ğŸ­ Los Cuatro Tipos de Drama\n",
    "\n",
    "En el teatro del Machine Learning, hay cuatro tipos de drama:\n",
    "- **True Positives**: Las victorias bien predichas (Â¡Ã©xito!)\n",
    "- **True Negatives**: Las derrotas bien predichas (Â¡tambiÃ©n Ã©xito!)\n",
    "- **False Positives**: Predijimos victoria pero hubo derrota (Â¡optimismo excesivo!)\n",
    "- **False Negatives**: Predijimos derrota pero hubo victoria (Â¡pesimismo injustificado!)\n",
    "\n",
    "### ğŸŒŠ La Zona de Incertidumbre\n",
    "\n",
    "Hay batallas que son genuinamente difÃ­ciles de predecir - aquellas donde nuestro modelo dice \"no estoy seguro\" (probabilidades cerca de 0.5). **Â¿QuÃ© hace que estas batallas sean tan impredecibles?** Â¿Son genuinamente aleatorias o hay patrones sutiles que aÃºn no capturamos?\n",
    "\n",
    "### ğŸ”¬ AnatomÃ­a de un Error\n",
    "\n",
    "Vamos a diseccionar nuestros errores como cientÃ­ficos forenses:\n",
    "- **Â¿En quÃ© caracterÃ­sticas difieren los casos mal clasificados?**\n",
    "- **Â¿Hay patrones en las probabilidades de predicciÃ³n?**\n",
    "- **Â¿Algunos tipos de batalla son mÃ¡s difÃ­ciles que otros?**\n",
    "\n",
    "**Â¿QuÃ© secretos revelarÃ¡n nuestros errores?** A veces, los fracasos enseÃ±an mÃ¡s que los Ã©xitosâ€¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de errores\n",
    "all_results = {**results, **optimized_results}\n",
    "error_analysis = trainer.analyze_prediction_errors(all_results, X_test, y_test, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98010451",
   "metadata": {},
   "source": [
    "## ğŸ“Š El Momento de la Verdad: Â¿Hemos Hecho Historia?\n",
    "\n",
    "**SeÃ±oras y seÃ±ores, el momento que todos hemos estado esperando ha llegado.** DespuÃ©s de horas de entrenamiento, optimizaciÃ³n y anÃ¡lisis, es hora de responder la pregunta fundamental:\n",
    "\n",
    "### ğŸ¯ Â¿Hemos Superado lo Imposible?\n",
    "\n",
    "Nuestro baseline de **ROC-AUC 0.837** ha sido nuestro dragÃ³n final desde el principio. Un nÃºmero que parecÃ­a formidable, casi inalcanzable. Pero hemos reunido el mejor arsenal de Machine Learning disponible y lo hemos lanzado contra este desafÃ­o.\n",
    "\n",
    "### ğŸ† El Podio de Campeones\n",
    "\n",
    "Como en cualquier competencia Ã©pica, vamos a coronar a nuestros campeones:\n",
    "- **ğŸ¥‡ Medalla de Oro**: El modelo supremo que reinarÃ¡ sobre todos\n",
    "- **ğŸ¥ˆ Medalla de Plata**: El digno segundo lugar\n",
    "- **ğŸ¥‰ Medalla de Bronce**: El tercer puesto honorable\n",
    "\n",
    "### ğŸ“ˆ La Historia en NÃºmeros\n",
    "\n",
    "Pero esto no es solo sobre ganar o perder. Es sobre **cuÃ¡nto hemos mejorado**. Â¿Fue una mejora marginal del 1%? Â¿O logramos un salto cuÃ¡ntico del 10% o mÃ¡s?\n",
    "\n",
    "### ğŸ­ El Drama del Resultado\n",
    "\n",
    "**Â¿CuÃ¡l serÃ¡ el veredicto final?** Â¿Celebraremos una victoria aplastante sobre el baseline? Â¿O descubriremos que el baseline era mÃ¡s formidable de lo que pensÃ¡bamos?\n",
    "\n",
    "**Â¿HabrÃ¡ sorpresas?** Â¿AlgÃºn modelo underdog que nadie esperaba se alzarÃ¡ como campeÃ³n? Â¿O el favorito cumplirÃ¡ las expectativas?\n",
    "\n",
    "**El suspenso estÃ¡ matando... Â¡Veamos los resultados!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte final\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† REPORTE FINAL - POKEMON BATTLE AI ML TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_auc = 0.837\n",
    "print(f\"\\nğŸ“Š BASELINE ROC-AUC: {baseline_auc:.4f}\")\n",
    "print(f\"ğŸ¯ MEJOR MODELO: {trainer.best_model}\")\n",
    "print(f\"ğŸ† MEJOR ROC-AUC: {trainer.best_score:.4f}\")\n",
    "\n",
    "improvement = ((trainer.best_score - baseline_auc) / baseline_auc) * 100\n",
    "if trainer.best_score > baseline_auc:\n",
    "    print(f\"âœ… MEJORA: +{improvement:.2f}% sobre baseline\")\n",
    "else:\n",
    "    print(f\"âŒ RENDIMIENTO: {improvement:.2f}% respecto al baseline\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ RESUMEN DE TODOS LOS MODELOS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Combinar todos los resultados\n",
    "all_model_scores = []\n",
    "\n",
    "# Modelos base\n",
    "for name, result in results.items():\n",
    "    all_model_scores.append((name, result['metrics']['roc_auc'], result['metrics']['mcc']))\n",
    "\n",
    "# Modelos optimizados\n",
    "for name, result in optimized_results.items():\n",
    "    all_model_scores.append((name, result['metrics']['roc_auc'], result['metrics']['mcc']))\n",
    "\n",
    "# Ordenar por ROC-AUC\n",
    "all_model_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (name, auc, mcc) in enumerate(all_model_scores, 1):\n",
    "    status = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\" if i == 3 else \"  \"\n",
    "    vs_baseline = \"âœ…\" if auc > baseline_auc else \"âŒ\"\n",
    "    print(f\"{status} {name:25} | ROC-AUC: {auc:.4f} | MCC: {mcc:.4f} | {vs_baseline}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa9da0",
   "metadata": {},
   "source": [
    "### ğŸ’¾ Preservando la Historia: El Legado de Nuestros Campeones\n",
    "\n",
    "Como arqueÃ³logos del futuro, debemos preservar cuidadosamente nuestros descubrimientos. El modelo campeÃ³n que hemos creado no es solo cÃ³digo - es **historia en el making**, el resultado de un viaje Ã©pico de descubrimiento y optimizaciÃ³n.\n",
    "\n",
    "### ğŸ›ï¸ El Museo de Nuestros Logros\n",
    "\n",
    "Vamos a crear un \"museo digital\" completo de nuestro proyecto:\n",
    "- **El Modelo CampeÃ³n**: Serializado y listo para la posteridad\n",
    "- **Los Resultados Completos**: Cada mÃ©trica, cada comparaciÃ³n, cada insight\n",
    "- **El Scaler**: Si nuestro campeÃ³n lo necesita, tambiÃ©n lo preservamos\n",
    "- **Los Metadatos**: La fecha, las condiciones, el contexto completo\n",
    "\n",
    "### ğŸ“œ El Pergamino de los Resultados\n",
    "\n",
    "Nuestro archivo JSON serÃ¡ como un pergamino antiguo que cuenta la historia completa:\n",
    "- Â¿QuiÃ©n fue el campeÃ³n?\n",
    "- Â¿CuÃ¡l fue su puntuaciÃ³n final?\n",
    "- Â¿CuÃ¡nto mejorÃ³ sobre el baseline?\n",
    "- Â¿CuÃ¡les fueron las caracterÃ­sticas mÃ¡s importantes?\n",
    "- Â¿CuÃ¡ndo ocurriÃ³ este momento histÃ³rico?\n",
    "\n",
    "### ğŸš€ Listo para la ProducciÃ³n\n",
    "\n",
    "Este no es el final de nuestro viaje - es el **comienzo de una nueva era**. Nuestro modelo campeÃ³n estÃ¡ listo para:\n",
    "- Predecir batallas Pokemon en tiempo real\n",
    "- Ayudar a entrenadores a tomar mejores decisiones\n",
    "- Revelar patrones ocultos en el mundo competitivo Pokemon\n",
    "\n",
    "**Â¡La historia ha sido escrita, el legado estÃ¡ asegurado!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de salida - detectar entorno\n",
    "if IS_KAGGLE:\n",
    "    output_dir = Path(f\"{WORKING_DIR}/models/trained\")\n",
    "else:\n",
    "    output_dir = Path(\"../models/trained\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Guardar mejor modelo\n",
    "if trainer.best_model in all_results:\n",
    "    best_model_obj = all_results[trainer.best_model]['model']\n",
    "    \n",
    "    # Guardar modelo\n",
    "    model_path = output_dir / f\"best_model_{trainer.best_model}.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(best_model_obj, f)\n",
    "    \n",
    "    # Guardar scaler si es necesario\n",
    "    if trainer.best_model in ['neural_network', 'svm', 'logistic_regression']:\n",
    "        scaler_path = output_dir / f\"scaler_{trainer.best_model}.pkl\"\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(trainer.scaler, f)\n",
    "    \n",
    "    print(f\"âœ… Mejor modelo guardado: {model_path}\")\n",
    "\n",
    "# Guardar resultados completos\n",
    "results_data = {\n",
    "    'best_model': trainer.best_model,\n",
    "    'best_score': trainer.best_score,\n",
    "    'baseline_auc': baseline_auc,\n",
    "    'improvement': improvement,\n",
    "    'feature_names': feature_names,\n",
    "    'model_scores': all_model_scores,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "results_path = output_dir / \"training_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Resultados guardados: {results_path}\")\n",
    "\n",
    "# Generar Ã­ndice de grÃ¡ficos para documentaciÃ³n tÃ©cnica\n",
    "trainer.generate_plots_index()\n",
    "\n",
    "print(\"\\nğŸ‰ Â¡Entrenamiento completado exitosamente!\")\n",
    "print(\"ğŸš€ El modelo estÃ¡ listo para hacer predicciones en batallas Pokemon!\")\n",
    "print(\"ğŸ“Š Todos los grÃ¡ficos han sido exportados para documentaciÃ³n tÃ©cnica\")\n",
    "\n",
    "# Mostrar resumen de archivos generados\n",
    "print(f\"\\nğŸ“ ARCHIVOS GENERADOS EN {output_dir.parent}:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Listar archivos de modelos\n",
    "if output_dir.exists():\n",
    "    model_files = list(output_dir.glob(\"*\"))\n",
    "    if model_files:\n",
    "        print(\"ğŸ¤– MODELOS:\")\n",
    "        for file in model_files:\n",
    "            print(f\"   ğŸ“„ {file.name}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No se encontraron archivos de modelos\")\n",
    "\n",
    "# Listar archivos de grÃ¡ficos\n",
    "if IS_KAGGLE:\n",
    "    plots_dir = Path(f\"{WORKING_DIR}/plots\")\n",
    "else:\n",
    "    plots_dir = Path(\"../plots\")\n",
    "\n",
    "if plots_dir.exists():\n",
    "    plot_files = list(plots_dir.glob(\"*.png\"))\n",
    "    if plot_files:\n",
    "        print(\"\\nğŸ“Š GRÃFICOS:\")\n",
    "        for file in plot_files:\n",
    "            print(f\"   ğŸ–¼ï¸  {file.name}\")\n",
    "    \n",
    "    # Mostrar README de plots si existe\n",
    "    readme_plots = plots_dir / \"README_PLOTS.md\"\n",
    "    if readme_plots.exists():\n",
    "        print(f\"   ğŸ“‹ README_PLOTS.md\")\n",
    "else:\n",
    "    print(\"âš ï¸  No se encontraron archivos de grÃ¡ficos\")\n",
    "\n",
    "print(f\"\\nğŸŒ Entorno: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "print(f\"ğŸ“‚ Directorio base: {WORKING_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
