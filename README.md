# ğŸ”¥ Pokemon Battle AI - El Camino hacia el Maestro Definitivo

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-red.svg)](https://pytorch.org/)
[![Reinforcement Learning](https://img.shields.io/badge/RL-DQN%20%7C%20PPO-red.svg)](https://pytorch.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3%2B-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Phase%201%20Complete-green.svg)](notebooks/)
[![Vision](https://img.shields.io/badge/Vision-RL%20Agent-purple.svg)](README.md)
[![ROC-AUC](https://img.shields.io/badge/ROC--AUC-0.837-brightgreen.svg)](output/baseline_model_performance.png)

![Pokemon Battle Analysis](assets/images/battle_patterns_analysis.png)

## ğŸ¯ La MisiÃ³n Ã‰pica: Crear el Maestro Pokemon Definitivo

**Un agente de Reinforcement Learning que juega batallas Pokemon, aprende de cada decisiÃ³n y evoluciona continuamente.** Este no es un simple predictor: es un **entrenador artificial** que toma decisiones en tiempo real, explora estrategias, comete errores, aprende de ellos y mejora hasta alcanzar el nivel de los mejores jugadores humanos.

### ğŸŒŸ La VisiÃ³n Final

- âœ¨ **Juega batallas completas** tomando decisiones turno a turno
- ğŸ§  **Aprende de cada acciÃ³n** mediante Reinforcement Learning
- ğŸ”„ **Se entrena mediante self-play** contra versiones de sÃ­ misma
- ğŸ“ˆ **Mejora continuamente** con cada batalla jugada
- ğŸ¯ **Explica sus decisiones** con razonamiento estratÃ©gico
- ğŸ† **Compite contra humanos** y aprende de maestros Pokemon

## ğŸ“Š El Arsenal de Datos: La Memoria de 14,000 Batallas

**La Biblioteca Completa de Experiencia Pokemon:**

- **Fuente**: Batallas reales de Pokemon Showdown (formato gen9randombattle)
- **Escala**: ~14,000 batallas individuales con logs completos
- **Formato**: JSON estructurado con secuencias de decisiones turno a turno

## ğŸ“Œ Resultados actuales (dataset completo)

- Baseline ROC-AUC 0.837 (dataset completo). Ver `output/baseline_model_performance.png`.

### ğŸ® Base de Datos Pokemon (`pokemon_data.py`)

**El corazÃ³n del sistema de type matchups:**

- **200+ Pokemon** con especies mapeadas (Gen 1-9)
- **Matriz 18x18** de efectividad de tipos completa
- **Base Stat Totals (BST)** para todos los Pokemon
- **Tiers competitivos**: Uber, OU, UU, RU
- **Funciones helper**: `get_pokemon_types()`, `get_pokemon_bst()`, `calculate_matchup_score()`

**Especies incluidas:**

- âœ… Todos los starters (Gen 1-9)
- âœ… Todos los legendarios principales
- âœ… Todos los pseudo-legendarios
- âœ… Pokemon competitivos populares
- âœ… Gen 9 completo (Paldea)

## ğŸš€ Arquitectura del Sistema: Del AnÃ¡lisis al Agente

### ğŸ”¬ Fase 1: Fundamentos (Completado)

**Sistema de PredicciÃ³n (Baseline):**

- âœ… Pipeline de datos: JSON â†’ Features â†’ Baseline ligero (p.ej., Logistic Regression)
- âœ… MÃ©trica principal: ROC-AUC (baseline de referencia)
- âœ… Visualizaciones temÃ¡ticas Pokemon (EDA)
- âœ… Base de datos Pokemon con tipos y BST
- ğŸ“ **Resultado**: PredicciÃ³n funcional, patrones identificados

### ğŸ¤– Fase 2-4: Agente RL (Roadmap)

- Agente DQN/PPO con PyTorch y memoria de experiencias.
- Entorno estilo Gym basado en estados de Showdown y acciones vÃ¡lidas.
- Recompensas: victoria/derrota + shaping estratÃ©gico.
- Entrenamiento: self-play, checkpoints, evaluaciÃ³n continua.
- AnÃ¡lisis: winrate y estrategias aprendidas vs humanos/baselines.

## ğŸ—ï¸ Estructura del Proyecto

### Estructura actual

```text
Pokemon_battle/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”‚   â””â”€â”€ random_agent.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ feature_extractor.py
â”‚   â”‚   â””â”€â”€ pokemon_data.py
â”‚   â”œâ”€â”€ environment/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ networks.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ battles/
â”‚   â””â”€â”€ archive.zip
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA_notebook_ready.ipynb / .py
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ images/
â”œâ”€â”€ output/
â”œâ”€â”€ tests/
â””â”€â”€ README.md
```

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Python 3.8+
- Git
- 4GB RAM mÃ­nimo (8GB recomendado)
- GPU opcional (para entrenamiento acelerado)

### InstalaciÃ³n Completa

```bash
# Clonar repositorio
git clone https://github.com/AlexGHerrera/Pokemon-battle-ai.git
cd Pokemon-battle-ai

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Crear directorios necesarios
python config/config.py
```

### Variables de Entorno (Opcional)

```bash
# .env
USE_GPU=true          # Usar GPU para entrenamiento
DEBUG=false           # Modo debug del servidor
DATA_SAMPLE_SIZE=2000 # TamaÃ±o de muestra para desarrollo
```

## ğŸš€ GuÃ­a de Uso

### ğŸ“Š Fase 1: Toma de Contacto (Completado)

**1. AnÃ¡lisis Exploratorio de Datos:**

```bash
# Ejecutar EDA completo
jupyter lab notebooks/EDA_notebook_ready.ipynb
```

**QuÃ© descubrirÃ¡s:**

- Patrones de victoria en 14,000+ batallas
- Correlaciones clave entre features vÃ¡lidas

### ğŸ”¬ Fase 2: AnÃ¡lisis de Decisiones (PrÃ³ximo Paso)

**Crear notebook de anÃ¡lisis de decisiones:**

```bash
# Analizar decisiones turno a turno
jupyter lab notebooks/EDA_Decision_Analysis.ipynb
```

**Objetivos:**

- Extraer secuencias (estado, acciÃ³n, resultado)
- DiseÃ±ar espacio de estados y acciones para RL

## ğŸ“Š Componentes del Sistema

### Fase 1 (Implementado)

- âœ… **FeatureExtractor** (`src/data/feature_extractor.py`): ExtracciÃ³n de features vÃ¡lidas
- âœ… **Type Matchup System**: CÃ¡lculo de efectividad elemental 18x18
- âœ… **EDA Visualization Suite**: GrÃ¡ficos temÃ¡ticos y anÃ¡lisis exploratorio

### Fase 2+ (A Implementar)

- â¸ï¸ **BattleParser**: Secuencias de decisiones para RL
- â¸ï¸ **PokemonBattleEnv**: Entorno Gym con acciones vÃ¡lidas
- â¸ï¸ **DQNAgent / PPOAgent**: Agentes de RL
- â¸ï¸ **RLTrainer**: Entrenamiento con self-play y evaluaciÃ³n

### Archivos Generados

**Fase 1:**

- `output/battle_features.csv` - Features extraÃ­das
- `output/*.png` - Visualizaciones EDA

**Fase 3+ (Futuro):**

- `data/rl_experiences/` - Experiencias del agente
- `src/models/pretrained/agent_*.pth` - Checkpoints del agente
- `logs/training_*.log` - Logs de entrenamiento RL

## ğŸ¯ Visualizaciones clave

- `assets/images/battle_patterns_analysis.png` â€” Ritmo de batalla y eventos.
- `assets/images/type_analysis.png` â€” Fuerzas elementales y winrates por tipo.
- `assets/images/correlation_matrix_filtered.png` â€” Relaciones entre features vÃ¡lidas.

## ğŸ“ˆ Roadmap: El Camino hacia el Maestro Pokemon

### âœ… Fase 1: Toma de Contacto (Completado)

**Objetivo:** Entender el dominio y validar que los patrones existen

- âœ… EDA de 14,000+ batallas
- âœ… Sistema de type matchups
- âœ… Base de datos Pokemon (200+ especies)
- âœ… Visualizaciones temÃ¡ticas
- ğŸ“ **ConclusiÃ³n:** Los patrones de victoria son predecibles

### ğŸ”„ Fase 2: AnÃ¡lisis de Decisiones (En Progreso)

**Objetivo:** Entender quÃ© decisiones llevan a la victoria

- ğŸ¯ Crear `EDA_Decision_Analysis.ipynb`
- ğŸ” Extraer secuencias (estado, acciÃ³n, resultado) de batallas
- ğŸ§  Analizar movimientos exitosos por situaciÃ³n
- ğŸ“Š Identificar patrones en secuencias de acciones
- ğŸ® DiseÃ±ar espacio de estados y acciones para RL
- ğŸ“ˆ Estudiar estrategias de jugadores top vs random

### â¸ï¸ Fase 3: Primer Agente RL (Pendiente)

**Objetivo:** Crear un agente que aprenda a jugar desde cero

1. **Implementar Entorno de Batalla**
   - Gym environment compatible con OpenAI Gym
   - RepresentaciÃ³n de estados de batalla
   - Sistema de acciones vÃ¡lidas
   - CÃ¡lculo de recompensas

2. **Implementar Agente DQN**
   - Red neuronal para Q-values
   - Replay buffer para experiencias
   - Target network para estabilidad
   - Epsilon-greedy para exploraciÃ³n

3. **Sistema de Self-Play**
   - Entrenamiento agente vs agente
   - Guardado de checkpoints
   - MonitorizaciÃ³n de progreso
   - EvaluaciÃ³n contra baselines

4. **IntegraciÃ³n con Pokemon Showdown**
   - API para jugar batallas reales
   - Parser de estados de batalla
   - Sistema de acciones

**ğŸ¯ Objetivo:** Winrate > 50% contra jugadores random

### â¸ï¸ Fase 4: EvoluciÃ³n y MaestrÃ­a (Futuro)

**Objetivo:** Alcanzar nivel competitivo humano

1. **Algoritmos Avanzados**
   - PPO (Proximal Policy Optimization)
   - A3C (Asynchronous Actor-Critic)
   - AlphaZero-style (MCTS + Neural Networks)

2. **Mejoras de Arquitectura**
   - Attention mechanisms para focus en Pokemon clave
   - LSTM para memoria de secuencias
   - Embeddings de Pokemon y movimientos

3. **Curriculum Learning**
   - Empezar contra oponentes dÃ©biles
   - Incrementar dificultad progresivamente
   - Aprender de jugadores humanos top

4. **Explicabilidad**
   - VisualizaciÃ³n de decisiones
   - AnÃ¡lisis de estrategias aprendidas
   - GeneraciÃ³n de narrativas de batalla

**ğŸ¯ Objetivo:** ELO 1500+ en Pokemon Showdown

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crea un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ‘¥ Autores

- **Alejandro Guerra Herrera** - *Desarrollo inicial* - [GitHub](https://github.com/AlexGHerrera)

## ğŸ™ Agradecimientos Ã‰picos

- **Pokemon Showdown** por ser la fuente de nuestras 14,000+ batallas Ã©picas
- **Comunidad Pokemon competitivo** por crear las estrategias que analizamos
- **HackABoss** por proporcionar el escenario para esta aventura Ã©pica
- **Satoshi Tajiri** por crear el universo Pokemon que inspirÃ³ este proyecto
- **Todos los entrenadores** cuyas batallas alimentan nuestros algoritmos

### ğŸ­ **FilosofÃ­a del Proyecto**

> *"En cada dataset hay una historia esperando ser contada. En cada algoritmo hay un gladiador esperando su momento de gloria. En cada predicciÃ³n hay una decisiÃ³n que puede cambiar el curso de una batalla."*
>
> **â€” El Manifiesto del Pokemon Battle AI**

## ğŸ“ Contacto

Para preguntas o colaboraciones:

- **Email**: <alex_gh@live.com>
- **LinkedIn**: [Alejandro Guerra Herrera](https://www.linkedin.com/in/alejandro-guerra-herrera-a86053115/)
- **GitHub**: [@AlexGHerrera](https://github.com/AlexGHerrera)

---

## ğŸŒŸ **Â¡Ãšnete a la Leyenda!**

â­ **Â¡Dale una estrella si este proyecto Ã©pico te ha inspirado!** â­

**Â¿Te atreves a crear un agente que aprenda a jugar Pokemon?**  
**Â¿LograrÃ¡s que supere a jugadores humanos?**  
**Â¿DescubrirÃ¡s estrategias que ni los maestros Pokemon conocen?**

### ğŸ”¥ **La aventura apenas comienza...**

*Fase 1 completada: Sabemos que los patrones existen.*  
*Fase 2 en progreso: Entendiendo las decisiones ganadoras.*  
*Fase 3+: Crear el agente que juegue y aprenda.*

**Â¡El camino hacia el Maestro Pokemon AI estÃ¡ trazado!** ğŸš€
