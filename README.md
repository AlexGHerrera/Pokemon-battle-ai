# üî• Pokemon Battle AI - El Camino hacia el Maestro Definitivo

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-red.svg)](https://pytorch.org/)
[![Reinforcement Learning](https://img.shields.io/badge/RL-DQN%20%7C%20PPO-red.svg)](https://pytorch.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3%2B-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Phase%201%20Complete-green.svg)](notebooks/)
[![Vision](https://img.shields.io/badge/Vision-RL%20Agent-purple.svg)](README.md)
[![ROC-AUC](https://img.shields.io/badge/ROC--AUC-0.837-brightgreen.svg)](output/baseline_model_performance.png)

![Pokemon Battle Analysis](assets/images/battle_patterns_analysis.png)

## üéØ La Misi√≥n √âpica: Crear el Maestro Pokemon Definitivo

**Un agente de Reinforcement Learning que juega batallas Pokemon, aprende de cada decisi√≥n y evoluciona continuamente.** Este no es un simple predictor: es un **entrenador artificial** que toma decisiones en tiempo real, explora estrategias, comete errores, aprende de ellos y mejora hasta alcanzar el nivel de los mejores jugadores humanos.

### üåü La Visi√≥n Final

- ‚ú® **Juega batallas completas** tomando decisiones turno a turno
- üß† **Aprende de cada acci√≥n** mediante Reinforcement Learning
- üîÑ **Se entrena mediante self-play** contra versiones de s√≠ misma
- üìà **Mejora continuamente** con cada batalla jugada
- üéØ **Explica sus decisiones** con razonamiento estrat√©gico
- üèÜ **Compite contra humanos** y aprende de maestros Pokemon

## üìä El Arsenal de Datos: La Memoria de 14,000 Batallas

**La Biblioteca Completa de Experiencia Pokemon:**

- **Fuente**: Batallas reales de Pokemon Showdown (formato gen9randombattle)
- **Escala**: ~14,000 batallas individuales con logs completos
- **Formato**: JSON estructurado con secuencias de decisiones turno a turno

## üìå Resultados actuales (dataset completo)

- Baseline ROC-AUC 0.837 (dataset completo). Ver `output/baseline_model_performance.png`.

### üéÆ Base de Datos Pokemon (`pokemon_data.py`)

**El coraz√≥n del sistema de type matchups:**

- **200+ Pokemon** con especies mapeadas (Gen 1-9)
- **Matriz 18x18** de efectividad de tipos completa
- **Base Stat Totals (BST)** para todos los Pokemon
- **Tiers competitivos**: Uber, OU, UU, RU
- **Funciones helper**: `get_pokemon_types()`, `get_pokemon_bst()`, `calculate_matchup_score()`

**Especies incluidas:**

- ‚úÖ Todos los starters (Gen 1-9)
- ‚úÖ Todos los legendarios principales
- ‚úÖ Todos los pseudo-legendarios
- ‚úÖ Pokemon competitivos populares
- ‚úÖ Gen 9 completo (Paldea)

## üöÄ Arquitectura del Sistema: Del An√°lisis al Agente

### üî¨ Fase 1: Fundamentos (Completado)

**Sistema de Predicci√≥n (Baseline):**

- ‚úÖ Pipeline de datos: JSON ‚Üí Features ‚Üí Baseline ligero (p.ej., Logistic Regression)
- ‚úÖ M√©trica principal: ROC-AUC (baseline de referencia)
- ‚úÖ Visualizaciones tem√°ticas Pokemon (EDA)
- ‚úÖ Base de datos Pokemon con tipos y BST
- üìù **Resultado**: Predicci√≥n funcional, patrones identificados

### ü§ñ Fase 2-4: Agente RL (Roadmap)

- Agente DQN/PPO con PyTorch y memoria de experiencias.
- Entorno estilo Gym basado en estados de Showdown y acciones v√°lidas.
- Recompensas: victoria/derrota + shaping estrat√©gico.
- Entrenamiento: self-play, checkpoints, evaluaci√≥n continua.
- An√°lisis: winrate y estrategias aprendidas vs humanos/baselines.

## üèóÔ∏è Estructura del Proyecto

### Estructura actual

```text
Pokemon_battle/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ random_agent.py
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pokemon_data.py
‚îÇ   ‚îú‚îÄ‚îÄ environment/
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ networks.py
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ battles/
‚îÇ   ‚îî‚îÄ‚îÄ archive.zip
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ EDA_notebook_ready.ipynb / .py
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îú‚îÄ‚îÄ output/
‚îú‚îÄ‚îÄ tests/
‚îî‚îÄ‚îÄ README.md
```

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.8+
- Git
- 4GB RAM m√≠nimo (8GB recomendado)
- GPU opcional (para entrenamiento acelerado)

### Instalaci√≥n Completa

```bash
# Clonar repositorio
git clone https://github.com/AlexGHerrera/Pokemon-battle-ai.git
cd Pokemon-battle-ai

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Crear directorios necesarios
python config/config.py
```

### Variables de Entorno (Opcional)

```bash
# .env
USE_GPU=true          # Usar GPU para entrenamiento
DEBUG=false           # Modo debug del servidor
DATA_SAMPLE_SIZE=2000 # Tama√±o de muestra para desarrollo
```

## üöÄ Gu√≠a de Uso

### üìä Fase 1: Toma de Contacto (Completado)

**1. An√°lisis Exploratorio de Datos:**

```bash
# Ejecutar EDA completo
jupyter lab notebooks/EDA_notebook_ready.ipynb
```

**Qu√© descubrir√°s:**

- Patrones de victoria en 14,000+ batallas
- Correlaciones clave entre features v√°lidas

### üî¨ Fase 2: An√°lisis de Decisiones (Pr√≥ximo Paso)

**Crear notebook de an√°lisis de decisiones:**

```bash
# Analizar decisiones turno a turno
jupyter lab notebooks/EDA_Decision_Analysis.ipynb
```

**Objetivos:**

- Extraer secuencias (estado, acci√≥n, resultado)
- Dise√±ar espacio de estados y acciones para RL

## üìä Componentes del Sistema

### Fase 1 (Implementado)

- ‚úÖ **FeatureExtractor** (`src/data/feature_extractor.py`): Extracci√≥n de features v√°lidas
- ‚úÖ **Type Matchup System**: C√°lculo de efectividad elemental 18x18
- ‚úÖ **EDA Visualization Suite**: Gr√°ficos tem√°ticos y an√°lisis exploratorio

### Fase 2+ (A Implementar)

- ‚è∏Ô∏è **BattleParser**: Secuencias de decisiones para RL
- ‚è∏Ô∏è **PokemonBattleEnv**: Entorno Gym con acciones v√°lidas
- ‚è∏Ô∏è **DQNAgent / PPOAgent**: Agentes de RL
- ‚è∏Ô∏è **RLTrainer**: Entrenamiento con self-play y evaluaci√≥n

### Archivos Generados

**Fase 1:**

- `output/battle_features.csv` - Features extra√≠das
- `output/*.png` - Visualizaciones EDA

**Fase 3+ (Futuro):**

- `data/rl_experiences/` - Experiencias del agente
- `src/models/pretrained/agent_*.pth` - Checkpoints del agente
- `logs/training_*.log` - Logs de entrenamiento RL

## üéØ Visualizaciones clave

- `assets/images/battle_patterns_analysis.png` ‚Äî Ritmo de batalla y eventos.
- `assets/images/type_analysis.png` ‚Äî Fuerzas elementales y winrates por tipo.
- `assets/images/correlation_matrix_filtered.png` ‚Äî Relaciones entre features v√°lidas.

## üìà Roadmap: El Camino hacia el Maestro Pokemon

### ‚úÖ Fase 1: Toma de Contacto (Completado)

**Objetivo:** Entender el dominio y validar que los patrones existen

- ‚úÖ EDA de 14,000+ batallas
- ‚úÖ Sistema de type matchups
- ‚úÖ Base de datos Pokemon (200+ especies)
- ‚úÖ Visualizaciones tem√°ticas
- üìù **Conclusi√≥n:** Los patrones de victoria son predecibles

### üîÑ Fase 2: An√°lisis de Decisiones (En Progreso)

**Objetivo:** Entender qu√© decisiones llevan a la victoria

- üéØ Crear `EDA_Decision_Analysis.ipynb`
- üîç Extraer secuencias (estado, acci√≥n, resultado) de batallas
- üß† Analizar movimientos exitosos por situaci√≥n
- üìä Identificar patrones en secuencias de acciones
- üéÆ Dise√±ar espacio de estados y acciones para RL
- üìà Estudiar estrategias de jugadores top vs random

### ‚è∏Ô∏è Fase 3: Primer Agente RL (Pendiente)

**Objetivo:** Crear un agente que aprenda a jugar desde cero

1. **Implementar Entorno de Batalla**
   - Gym environment compatible con OpenAI Gym
   - Representaci√≥n de estados de batalla
   - Sistema de acciones v√°lidas
   - C√°lculo de recompensas

2. **Implementar Agente DQN**
   - Red neuronal para Q-values
   - Replay buffer para experiencias
   - Target network para estabilidad
   - Epsilon-greedy para exploraci√≥n

3. **Sistema de Self-Play**
   - Entrenamiento agente vs agente
   - Guardado de checkpoints
   - Monitorizaci√≥n de progreso
   - Evaluaci√≥n contra baselines

4. **Integraci√≥n con Pokemon Showdown**
   - API para jugar batallas reales
   - Parser de estados de batalla
   - Sistema de acciones

**üéØ Objetivo:** Winrate > 50% contra jugadores random

### ‚è∏Ô∏è Fase 4: Evoluci√≥n y Maestr√≠a (Futuro)

**Objetivo:** Alcanzar nivel competitivo humano

1. **Algoritmos Avanzados**
   - PPO (Proximal Policy Optimization)
   - A3C (Asynchronous Actor-Critic)
   - AlphaZero-style (MCTS + Neural Networks)

2. **Mejoras de Arquitectura**
   - Attention mechanisms para focus en Pokemon clave
   - LSTM para memoria de secuencias
   - Embeddings de Pokemon y movimientos

3. **Curriculum Learning**
   - Empezar contra oponentes d√©biles
   - Incrementar dificultad progresivamente
   - Aprender de jugadores humanos top

4. **Explicabilidad**
   - Visualizaci√≥n de decisiones
   - An√°lisis de estrategias aprendidas
   - Generaci√≥n de narrativas de batalla

**üéØ Objetivo:** ELO 1500+ en Pokemon Showdown

## ü§ù Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'A√±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crea un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## üë• Autores

- **Alejandro Guerra Herrera** - *Desarrollo inicial* - [GitHub](https://github.com/AlexGHerrera)

## üôè Agradecimientos √âpicos

- **Pokemon Showdown** por ser la fuente de nuestras 14,000+ batallas √©picas
- **Comunidad Pokemon competitivo** por crear las estrategias que analizamos
- **HackABoss** por proporcionar el escenario para esta aventura √©pica
- **Satoshi Tajiri** por crear el universo Pokemon que inspir√≥ este proyecto
- **Todos los entrenadores** cuyas batallas alimentan nuestros algoritmos

### üé≠ **Filosof√≠a del Proyecto**

> *"En cada dataset hay una historia esperando ser contada. En cada algoritmo hay un gladiador esperando su momento de gloria. En cada predicci√≥n hay una decisi√≥n que puede cambiar el curso de una batalla."*
>
> **‚Äî El Manifiesto del Pokemon Battle AI**

## üìû Contacto

Para preguntas o colaboraciones:

- **Email**: <alex_gh@live.com>
- **LinkedIn**: [Alejandro Guerra Herrera](https://www.linkedin.com/in/alejandro-guerra-herrera-a86053115/)
- **GitHub**: [@AlexGHerrera](https://github.com/AlexGHerrera)

---

## üåü **¬°√önete a la Leyenda!**

‚≠ê **¬°Dale una estrella si este proyecto √©pico te ha inspirado!** ‚≠ê

**¬øTe atreves a crear un agente que aprenda a jugar Pokemon?**  
**¬øLograr√°s que supere a jugadores humanos?**  
**¬øDescubrir√°s estrategias que ni los maestros Pokemon conocen?**

### üî• **La aventura apenas comienza...**

*Fase 1 completada: Sabemos que los patrones existen.*  
*Fase 2 en progreso: Entendiendo las decisiones ganadoras.*  
*Fase 3+: Crear el agente que juegue y aprenda.*

**¬°El camino hacia el Maestro Pokemon AI est√° trazado!** üöÄ
