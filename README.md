# ğŸ”¥ Pokemon Battle AI - El Camino hacia el Maestro Definitivo

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-red.svg)](https://pytorch.org/)
[![Reinforcement Learning](https://img.shields.io/badge/RL-DQN%20%7C%20PPO-red.svg)](https://pytorch.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3%2B-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Phase%201%20Complete-green.svg)](notebooks/)
[![Vision](https://img.shields.io/badge/Vision-RL%20Agent-purple.svg)](README.md)

![Pokemon Battle Analysis](assets/images/battle_patterns_analysis.png)

> **"En el mundo de las batallas Pokemon, cada decisiÃ³n cuenta. Cada movimiento, cada cambio, cada estrategia puede determinar la diferencia entre la victoria y la derrota. Nuestro objetivo: crear una IA que no solo prediga batallas, sino que las JUEGUE, aprenda de cada decisiÃ³n y se convierta en el mejor entrenador Pokemon."**

## ğŸ¯ La MisiÃ³n Ã‰pica: Crear el Maestro Pokemon Definitivo

**Un agente de Reinforcement Learning que juega batallas Pokemon, aprende de cada decisiÃ³n y evoluciona continuamente.** Este no es un simple predictor: es un **entrenador artificial** que toma decisiones en tiempo real, explora estrategias, comete errores, aprende de ellos y mejora hasta alcanzar el nivel de los mejores jugadores humanos.

### ğŸŒŸ La VisiÃ³n Final

Imagina una IA que:

- âœ¨ **Juega batallas completas** tomando decisiones turno a turno
- ğŸ§  **Aprende de cada acciÃ³n** mediante Reinforcement Learning
- ğŸ”„ **Se entrena mediante self-play** contra versiones de sÃ­ misma
- ğŸ“ˆ **Mejora continuamente** con cada batalla jugada
- ğŸ¯ **Explica sus decisiones** con razonamiento estratÃ©gico
- ğŸ† **Compite contra humanos** y aprende de maestros Pokemon

### ğŸ† El Viaje Ã‰pico: Estado Actual

**Fase 1: Toma de Contacto - Conociendo el Campo de Batalla** âœ… **COMPLETADO**

- âœ… AnÃ¡lisis exploratorio de 14,000+ batallas reales (`EDA_notebook_ready.ipynb`)
- âœ… Visualizaciones temÃ¡ticas que revelan patrones del meta-game
- âœ… ComprensiÃ³n profunda de tipos, Pokemon y mecÃ¡nicas
- âœ… Baseline predictor: ROC-AUC **0.819** - *"Â¿Puede un modelo predecir victorias?"*
- ğŸ“Š **Aprendizaje clave**: SÃ­, los patrones existen y son capturables

**Fase 2: AnÃ¡lisis de Decisiones - El Cerebro del Entrenador** ğŸ”„ **EN PROGRESO**

- ğŸ¯ Nuevo notebook: `EDA_Decision_Analysis.ipynb`
- ğŸ” AnÃ¡lisis turno a turno de decisiones exitosas
- ğŸ§  IdentificaciÃ³n de patrones en secuencias de movimientos
- ğŸ“ˆ DiseÃ±o del espacio de estados y acciones para RL
- ğŸ® ExtracciÃ³n de "jugadas ganadoras" de maestros Pokemon

**Fase 3: El Primer Agente - Aprendiendo a Jugar** â¸ï¸ **PENDIENTE**

- ğŸ¤– ImplementaciÃ³n de agente DQN (Deep Q-Network)
- ğŸ² Entrenamiento mediante self-play
- ğŸ“Š Sistema de recompensas y evaluaciÃ³n
- ğŸ”„ IntegraciÃ³n con Pokemon Showdown API
- ğŸ¯ Objetivo: Ganar > 50% contra jugadores random

**Fase 4: EvoluciÃ³n y MaestrÃ­a - El Camino al Top** â¸ï¸ **PENDIENTE**

- ğŸš€ Algoritmos avanzados: PPO, A3C, AlphaZero-style
- ğŸ§¬ OptimizaciÃ³n de hiperparÃ¡metros y arquitecturas
- ğŸ† CompeticiÃ³n contra jugadores humanos
- ğŸ“ˆ Aprendizaje continuo desde batallas en vivo
- ğŸ¯ Objetivo final: Alcanzar nivel competitivo (1500+ ELO)

### ğŸ“Š **Estado Actual del Proyecto**

**ğŸ¯ Fase 1 Completada - Toma de Contacto:**

- âœ… **EDA Exploratorio**: 14,000+ batallas analizadas
- âœ… **Baseline Predictor**: ROC-AUC 0.819 (prueba de concepto exitosa)
- âœ… **Feature Engineering**: 37 caracterÃ­sticas pre-batalla extraÃ­das
- âœ… **Type Matchups**: Sistema 18x18 tipos implementado
- âœ… **Pokemon Database**: 200+ especies mapeadas con BST
- ğŸ“ **ConclusiÃ³n**: Los patrones de victoria son predecibles, base sÃ³lida para RL

**ğŸ”¬ PrÃ³ximos Pasos - Fase 2:**

- ğŸ¯ Crear `EDA_Decision_Analysis.ipynb`
- ğŸ” Analizar decisiones turno a turno
- ğŸ§  DiseÃ±ar espacio de estados para agente RL
- ğŸ“Š Identificar patrones en secuencias de acciones
- ğŸ® Preparar datos para entrenamiento de agente

### ğŸ“Š Visualizaciones Generadas

**AnÃ¡lisis Exploratorio:**

- `battle_patterns_analysis.png` - Patrones de duraciÃ³n y eventos
- `pokemon_analysis.png` - Top Pokemon, niveles, HP
- `type_analysis.png` - DistribuciÃ³n y winrates por tipo
- `distributions_analysis.png` - Distribuciones y outliers
- `correlation_matrix_filtered.png` - Correlaciones entre features

**Modelo Baseline:**

- `baseline_model_performance.png` - ROC Curve (AUC=0.819) + Top 15 Features

## ğŸ“Š El Arsenal de Datos: La Memoria de 14,000 Batallas

**La Biblioteca Completa de Experiencia Pokemon:**

- **Fuente**: Batallas reales de Pokemon Showdown (formato gen9randombattle)
- **Escala**: ~14,000 batallas individuales con logs completos
- **Formato**: JSON estructurado con secuencias de decisiones turno a turno
- **Riqueza**: Movimientos, cambios, daÃ±o, estados, metadata de jugadores
- **Valor para RL**: Cada batalla es una secuencia de (estado, acciÃ³n, recompensa)
- **Uso actual**: Fase 1 (predicciÃ³n) â†’ Fase 2+ (entrenamiento de agente)

### ğŸ® Base de Datos Pokemon (`pokemon_data.py`)

**El corazÃ³n del sistema de type matchups:**

- **200+ Pokemon** con especies mapeadas (Gen 1-9)
- **Matriz 18x18** de efectividad de tipos completa
- **Base Stat Totals (BST)** para todos los Pokemon
- **Tiers competitivos**: Uber, OU, UU, RU
- **Funciones helper**: `get_pokemon_types()`, `get_pokemon_bst()`, `calculate_matchup_score()`

**Especies incluidas:**

- âœ… Todos los starters (Gen 1-9)
- âœ… Todos los legendarios principales
- âœ… Todos los pseudo-legendarios
- âœ… Pokemon competitivos populares
- âœ… Gen 9 completo (Paldea)

### ğŸ­ Los Protagonistas de Nuestra Historia

Cada batalla es un testimonio de:
- **Decisiones bajo presiÃ³n** de entrenadores reales
- **Estrategias complejas** ejecutadas en tiempo real
- **Momentos crÃ­ticos** que definen victoria o derrota
- **Patrones meta** que solo emergen con grandes volÃºmenes de datos

## ğŸš€ Arquitectura del Sistema: Del AnÃ¡lisis al Agente

### ğŸ”¬ Fase 1: Fundamentos (Completado)

**Sistema de PredicciÃ³n (Baseline):**

- âœ… Pipeline de datos: JSON â†’ Features â†’ Modelo
- âœ… 7 algoritmos ML probados (Logistic, RF, XGBoost, LightGBM, NN, SVM, GB)
- âœ… MÃ©tricas avanzadas: ROC-AUC, MCC, Brier Score
- âœ… Visualizaciones temÃ¡ticas Pokemon
- âœ… Base de datos Pokemon con tipos y BST
- ğŸ“ **Resultado**: PredicciÃ³n funcional, patrones identificados

### ğŸ¤– Fase 2-4: Agente RL (Roadmap)

**Componentes a Desarrollar:**

1. **ğŸ§  Agente de Reinforcement Learning**
   - Arquitecturas: DQN, PPO, A3C
   - Redes neuronales con PyTorch
   - Sistema de memoria y replay buffer
   - ExploraciÃ³n vs explotaciÃ³n (epsilon-greedy)

2. **ğŸ® Entorno de Batalla**
   - IntegraciÃ³n con Pokemon Showdown API
   - Simulador local para self-play
   - RepresentaciÃ³n de estados de batalla
   - Sistema de acciones (movimientos + cambios)

3. **ğŸ“Š Sistema de Recompensas**
   - Recompensa final: +1 victoria / -1 derrota
   - Recompensas intermedias: daÃ±o, KOs, ventaja
   - Penalizaciones: movimientos inÃºtiles, errores
   - Shaping para acelerar aprendizaje

4. **ğŸ”„ Pipeline de Entrenamiento**
   - Self-play: agente vs agente
   - EvaluaciÃ³n contra baselines
   - Guardado de checkpoints
   - MonitorizaciÃ³n de progreso

5. **ğŸ“Š AnÃ¡lisis y EvaluaciÃ³n**
   - Winrate vs diferentes oponentes
   - AnÃ¡lisis de estrategias aprendidas
   - VisualizaciÃ³n de decisiones
   - ComparaciÃ³n con jugadores humanos

## ğŸ—ï¸ Estructura del Proyecto

```text
Pokemon_battle/ ğŸ° El Camino hacia el Maestro Pokemon
â”œâ”€â”€ src/ âš”ï¸ CÃ³digo Principal
â”‚   â”œâ”€â”€ data/ ğŸ§¬ Procesamiento de Datos
â”‚   â”‚   â”œâ”€â”€ processors.py          # ExtracciÃ³n de features (Fase 1)
â”‚   â”‚   â”œâ”€â”€ battle_parser.py       # Parser de batallas para RL (Fase 2+)
â”‚   â”‚   â”œâ”€â”€ loaders/               # Carga de datos
â”‚   â”‚   â””â”€â”€ validators/            # ValidaciÃ³n de datos
â”‚   â”œâ”€â”€ models/ ğŸ¤– Modelos de IA
â”‚   â”‚   â”œâ”€â”€ architectures.py       # Redes neuronales (predictor + agente)
â”‚   â”‚   â”œâ”€â”€ rl_agent.py            # Agente RL (DQN/PPO) - Fase 3+
â”‚   â”‚   â””â”€â”€ pretrained/            # Modelos guardados
â”‚   â”œâ”€â”€ training/ ğŸŸï¸ Entrenamiento
â”‚   â”‚   â”œâ”€â”€ ml_trainer.py          # Entrenador ML clÃ¡sico (Fase 1)
â”‚   â”‚   â”œâ”€â”€ rl_trainer.py          # Entrenador RL (Fase 3+)
â”‚   â”‚   â””â”€â”€ self_play.py           # Sistema de self-play (Fase 3+)
â”‚   â”œâ”€â”€ environment/ ğŸ® Entorno de Batalla
â”‚   â”‚   â”œâ”€â”€ battle_env.py          # Gym environment (Fase 3+)
â”‚   â”‚   â”œâ”€â”€ showdown_api.py        # IntegraciÃ³n Pokemon Showdown (Fase 3+)
â”‚   â”‚   â””â”€â”€ simulator.py           # Simulador local (Fase 3+)
â”‚   â””â”€â”€ utils/ ğŸ› ï¸ Utilidades
â”œâ”€â”€ config/ âš™ï¸ ConfiguraciÃ³n
â”‚   â””â”€â”€ config.py                  # ConfiguraciÃ³n global
â”œâ”€â”€ data/ ğŸ’¾ Datos
â”‚   â”œâ”€â”€ battles/                   # 14,000+ batallas JSON
â”‚   â”œâ”€â”€ all_battles.json          # Dataset completo
â”‚   â”œâ”€â”€ battles_sample_2000.json  # Muestra para desarrollo
â”‚   â””â”€â”€ rl_experiences/            # Experiencias del agente (Fase 3+)
â”œâ”€â”€ notebooks/ ğŸ“š Notebooks de AnÃ¡lisis
â”‚   â”œâ”€â”€ EDA_notebook_ready.ipynb       # âœ… Fase 1: Toma de contacto
â”‚   â”œâ”€â”€ EDA_Decision_Analysis.ipynb    # ğŸ”„ Fase 2: AnÃ¡lisis de decisiones
â”‚   â”œâ”€â”€ ML_Training_Advanced.ipynb     # âœ… Fase 1: Baseline predictor
â”‚   â””â”€â”€ RL_Agent_Training.ipynb        # â¸ï¸ Fase 3+: Entrenamiento agente
â”œâ”€â”€ assets/ ğŸ¨ Visualizaciones
â”‚   â””â”€â”€ images/                    # GrÃ¡ficos y anÃ¡lisis
â”œâ”€â”€ tests/ ğŸ§ª Tests
â”œâ”€â”€ docs/ ğŸ“œ DocumentaciÃ³n
â””â”€â”€ logs/ ğŸ“ Logs del sistema
```

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Python 3.8+
- Git
- 4GB RAM mÃ­nimo (8GB recomendado)
- GPU opcional (para entrenamiento acelerado)

### InstalaciÃ³n Completa

```bash
# Clonar repositorio
git clone https://github.com/AlexGHerrera/Pokemon-battle-ai.git
cd Pokemon-battle-ai

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Crear directorios necesarios
python config/config.py
```

### Variables de Entorno (Opcional)

```bash
# .env
USE_GPU=true          # Usar GPU para entrenamiento
DEBUG=false           # Modo debug del servidor
DATA_SAMPLE_SIZE=2000 # TamaÃ±o de muestra para desarrollo
```

## ğŸš€ GuÃ­a de Uso

### ğŸ“Š Fase 1: Toma de Contacto (Completado)

**1. AnÃ¡lisis Exploratorio de Datos:**

```bash
# Ejecutar EDA completo
jupyter lab notebooks/EDA_notebook_ready.ipynb
```

**QuÃ© descubrirÃ¡s:**

- Patrones de victoria en 14,000+ batallas
- Pokemon y tipos mÃ¡s efectivos
- Correlaciones entre features
- Baseline predictor: ROC-AUC 0.819

**2. Entrenar Baseline Predictor (Opcional):**

```bash
# Entrenar 7 algoritmos ML
jupyter lab notebooks/ML_Training_Advanced.ipynb
```

**Resultado:**

- ComparaciÃ³n de 7 algoritmos
- Visualizaciones de rendimiento
- Modelo guardado para referencia

### ğŸ”¬ Fase 2: AnÃ¡lisis de Decisiones (PrÃ³ximo Paso)

**Crear notebook de anÃ¡lisis de decisiones:**

```bash
# Analizar decisiones turno a turno
jupyter lab notebooks/EDA_Decision_Analysis.ipynb
```

**Objetivos:**

- Extraer secuencias (estado, acciÃ³n, resultado)
- Identificar movimientos exitosos por situaciÃ³n
- DiseÃ±ar espacio de estados para RL
- Analizar estrategias de jugadores top

### ğŸ¤– Fase 3+: Entrenamiento de Agente RL (Futuro)

**Entrenar agente que juega batallas:**

```python
# Ejemplo conceptual (a implementar)
from src.models.rl_agent import DQNAgent
from src.environment.battle_env import PokemonBattleEnv
from src.training.rl_trainer import RLTrainer

# Crear entorno
env = PokemonBattleEnv()

# Crear agente
agent = DQNAgent(
    state_size=env.observation_space.shape[0],
    action_size=env.action_space.n
)

# Entrenar mediante self-play
trainer = RLTrainer(agent, env)
trainer.train(episodes=10000)
```

## ğŸ”§ ConfiguraciÃ³n (Roadmap)

### ConfiguraciÃ³n de Agente RL (Fase 3+)

```python
# config/config.py - A implementar
RL_CONFIG = {
    # Arquitectura del agente
    "agent_type": "DQN",  # DQN, PPO, A3C
    "state_size": 256,    # DimensiÃ³n del estado
    "action_size": 9,     # 4 movimientos + 5 cambios
    "hidden_layers": [512, 256, 128],
    
    # HiperparÃ¡metros de entrenamiento
    "learning_rate": 0.0001,
    "gamma": 0.99,        # Factor de descuento
    "epsilon_start": 1.0, # ExploraciÃ³n inicial
    "epsilon_end": 0.01,
    "epsilon_decay": 0.995,
    
    # Replay buffer
    "buffer_size": 100000,
    "batch_size": 64,
    
    # Self-play
    "episodes": 10000,
    "max_steps_per_episode": 100,
    "update_target_every": 1000,
}
```

### Sistema de Recompensas (Fase 3+)

```python
REWARD_CONFIG = {
    "win": 1.0,
    "loss": -1.0,
    "ko_opponent": 0.3,
    "lose_pokemon": -0.3,
    "damage_dealt": 0.001,  # Por punto de daÃ±o
    "damage_taken": -0.001,
    "invalid_move": -0.5,
    "type_advantage": 0.1,
}
```

## ğŸ“Š Componentes del Sistema

### Fase 1 (Implementado)

- âœ… **BattleDataProcessor**: Extrae features de batallas
- âœ… **PokemonMLTrainer**: Entrena 7 algoritmos ML
- âœ… **Type Matchup System**: CÃ¡lculo de efectividad
- âœ… **Visualization Suite**: GrÃ¡ficos temÃ¡ticos

### Fase 2+ (A Implementar)

- â¸ï¸ **BattleParser**: Extrae secuencias de decisiones
- â¸ï¸ **PokemonBattleEnv**: Gym environment para RL
- â¸ï¸ **DQNAgent / PPOAgent**: Agentes de RL
- â¸ï¸ **RLTrainer**: Sistema de entrenamiento RL
- â¸ï¸ **SelfPlaySystem**: Self-play para mejora continua
- â¸ï¸ **ShowdownAPI**: IntegraciÃ³n con Pokemon Showdown

### Archivos Generados

**Fase 1:**

- `output/battle_features.csv` - Features extraÃ­das
- `output/*.png` - Visualizaciones EDA
- `src/models/pretrained/baseline_*.pkl` - Modelos baseline

**Fase 3+ (Futuro):**

- `data/rl_experiences/` - Experiencias del agente
- `src/models/pretrained/agent_*.pth` - Checkpoints del agente
- `logs/training_*.log` - Logs de entrenamiento RL

## ğŸ¯ Insights Clave para IA

![Pokemon Usage Analysis](assets/images/pokemon_analysis.png)

### Patrones EstratÃ©gicos Identificados

- **Balance de jugadores**: DistribuciÃ³n equilibrada de victorias p1 vs p2
- **DuraciÃ³n Ã³ptima**: Batallas de 15-25 turnos muestran mayor complejidad estratÃ©gica
- **Meta dominante**: Top Pokemon mÃ¡s utilizados (Arceus, Rotom, Oricorio)
- **Eventos crÃ­ticos**: Ratio movimientos/switches indica agresividad vs cautela
- **Tipos dominantes**: Dragon (143 usos), Flying (134 usos), Poison (81 usos)
- **Mejores winrates**: Rock (51.1%), Dark (51.1%), Ghost (48.8%)

![Type Analysis](assets/images/type_analysis.png)

### Features VÃ¡lidas para ML (Sin Data Leakage)

**âœ… InformaciÃ³n PRE-BATALLA (Deployable en producciÃ³n):**

**1. Type Matchups** â­â­â­â­â­ (Factor #1 en Pokemon)

- `type_advantage_score`: Ventaja elemental general
- `super_effective_count`: CuÃ¡ntos Pokemon tienen ventaja de tipo
- `resisted_count`: CuÃ¡ntos ataques serÃ¡n resistidos
- `type_diversity`: Variedad de tipos en el equipo
- `dual_type_count`: Pokemon con doble tipo

**2. Pokemon Strength (BST)** â­â­â­â­

- `avg_bst`: Base Stat Total promedio del equipo
- `bst_advantage`: Ventaja de poder bruto
- `legendary_count`: NÃºmero de Pokemon legendarios
- `pseudo_legendary_count`: NÃºmero de pseudo-legendarios
- `min/max_bst`: Pokemon mÃ¡s dÃ©bil/fuerte

**3. ComposiciÃ³n Observable** â­â­â­â­

- `team_size`: TamaÃ±o del equipo
- `avg_level`: Nivel promedio de Pokemon
- `total_hp`: HP total disponible
- `species_diversity`: Diversidad de especies

**4. Ventajas Derivadas** â­â­â­

- `level_advantage`: Diferencia de niveles entre equipos
- `hp_advantage`: Diferencia de HP total
- `bst_advantage`: Diferencia de poder bruto

**âŒ NO USAMOS (SerÃ­a data leakage):**

- ~~`total_turns`~~ - Solo se conoce al final
- ~~`move_events`, `switch_events`~~ - Ocurren durante la batalla
- ~~`ladder_rating`~~ - No disponible en producciÃ³n

## ğŸ“Š Fase 1: Revelaciones del AnÃ¡lisis Exploratorio

### ğŸ­ La Historia que Cuentan los NÃºmeros

![Distributions Analysis](assets/images/distributions_analysis.png)

**Patrones Ã©picos descubiertos en nuestro viaje:**
- **DuraciÃ³n de batallas**: Media de 24.5 turnos (el ritmo perfecto del drama)
- **Eventos por turno**: CorrelaciÃ³n alta (0.981) con duraciÃ³n total
- **Outliers**: ~3-4% de batallas verdaderamente excepcionales
- **AnÃ¡lisis integrado**: Cada grÃ¡fico revela patrones estratÃ©gicos Pokemon

### ğŸ† Los Campeones Revelados

**Pokemon mÃ¡s utilizados (Los protagonistas):**
- **Arceus**: El dios Pokemon, omnipresente en batallas
- **Rotom**: El espÃ­ritu versÃ¡til que se adapta a todo
- **Oricorio**: El bailarÃ­n que sorprende con su presencia

**Tipos dominantes (Las fuerzas elementales):**
- **Dragon** (143 usos): Los legendarios reinan supremos
- **Flying** (134 usos): La libertad del cielo
- **Poison** (81 usos): La toxicidad estratÃ©gica

### Correlaciones de Features VÃ¡lidas

![Correlation Matrix](assets/images/correlation_matrix_filtered.png)

**Insights clave del anÃ¡lisis de correlaciones:**

- Type matchups y BST son altamente predictivos
- Features de composiciÃ³n observable muestran patrones claros
- No hay multicolinealidad problemÃ¡tica entre features vÃ¡lidas
- El modelo puede aprender relaciones complejas sin data leakage

### ConfiguraciÃ³n Avanzada

```python
# Muestra pequeÃ±a para pruebas rÃ¡pidas
battles = create_sample_dataset(sample_size=500)

# Muestra grande para anÃ¡lisis detallado
battles = create_sample_dataset(sample_size=5000)
```

## ğŸ“ˆ Roadmap: El Camino hacia el Maestro Pokemon

### âœ… Fase 1: Toma de Contacto (Completado)

**Objetivo:** Entender el dominio y validar que los patrones existen

- âœ… EDA de 14,000+ batallas
- âœ… Sistema de type matchups
- âœ… Base de datos Pokemon (200+ especies)
- âœ… Baseline predictor (ROC-AUC 0.819)
- âœ… Visualizaciones temÃ¡ticas
- ğŸ“ **ConclusiÃ³n:** Los patrones de victoria son predecibles

### ğŸ”„ Fase 2: AnÃ¡lisis de Decisiones (En Progreso)

**Objetivo:** Entender quÃ© decisiones llevan a la victoria

- ğŸ¯ Crear `EDA_Decision_Analysis.ipynb`
- ğŸ” Extraer secuencias (estado, acciÃ³n, resultado) de batallas
- ğŸ§  Analizar movimientos exitosos por situaciÃ³n
- ğŸ“Š Identificar patrones en secuencias de acciones
- ğŸ® DiseÃ±ar espacio de estados y acciones para RL
- ğŸ“ˆ Estudiar estrategias de jugadores top vs random

### â¸ï¸ Fase 3: Primer Agente RL (Pendiente)

**Objetivo:** Crear un agente que aprenda a jugar desde cero

1. **Implementar Entorno de Batalla**
   - Gym environment compatible con OpenAI Gym
   - RepresentaciÃ³n de estados de batalla
   - Sistema de acciones vÃ¡lidas
   - CÃ¡lculo de recompensas

2. **Implementar Agente DQN**
   - Red neuronal para Q-values
   - Replay buffer para experiencias
   - Target network para estabilidad
   - Epsilon-greedy para exploraciÃ³n

3. **Sistema de Self-Play**
   - Entrenamiento agente vs agente
   - Guardado de checkpoints
   - MonitorizaciÃ³n de progreso
   - EvaluaciÃ³n contra baselines

4. **IntegraciÃ³n con Pokemon Showdown**
   - API para jugar batallas reales
   - Parser de estados de batalla
   - Sistema de acciones

**ğŸ¯ Objetivo:** Winrate > 50% contra jugadores random

### â¸ï¸ Fase 4: EvoluciÃ³n y MaestrÃ­a (Futuro)

**Objetivo:** Alcanzar nivel competitivo humano

1. **Algoritmos Avanzados**
   - PPO (Proximal Policy Optimization)
   - A3C (Asynchronous Actor-Critic)
   - AlphaZero-style (MCTS + Neural Networks)

2. **Mejoras de Arquitectura**
   - Attention mechanisms para focus en Pokemon clave
   - LSTM para memoria de secuencias
   - Embeddings de Pokemon y movimientos

3. **Curriculum Learning**
   - Empezar contra oponentes dÃ©biles
   - Incrementar dificultad progresivamente
   - Aprender de jugadores humanos top

4. **Explicabilidad**
   - VisualizaciÃ³n de decisiones
   - AnÃ¡lisis de estrategias aprendidas
   - GeneraciÃ³n de narrativas de batalla

**ğŸ¯ Objetivo:** ELO 1500+ en Pokemon Showdown

### ğŸŒŸ VisiÃ³n Final: El Maestro Pokemon AI

Una IA que:

- âœ¨ **Juega batallas** a nivel competitivo humano
- ğŸ§  **Aprende continuamente** de cada batalla
- ğŸ”„ **Se adapta** a cambios en el meta-game
- ğŸ“Š **Explica sus decisiones** de forma comprensible
- ğŸ† **Compite** contra los mejores jugadores
- ğŸ’¡ **Descubre estrategias** que humanos no han considerado

## ğŸ¤ Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crea un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ‘¥ Autores

- **Alejandro Guerra Herrera** - *Desarrollo inicial* - [GitHub](https://github.com/AlexGHerrera)

## ğŸ™ Agradecimientos Ã‰picos

- **Pokemon Showdown** por ser la fuente de nuestras 14,000+ batallas Ã©picas
- **Comunidad Pokemon competitivo** por crear las estrategias que analizamos
- **HackABoss** por proporcionar el escenario para esta aventura Ã©pica
- **Satoshi Tajiri** por crear el universo Pokemon que inspirÃ³ este proyecto
- **Todos los entrenadores** cuyas batallas alimentan nuestros algoritmos

### ğŸ­ **FilosofÃ­a del Proyecto**

> *"En cada dataset hay una historia esperando ser contada. En cada algoritmo hay un gladiador esperando su momento de gloria. En cada predicciÃ³n hay una decisiÃ³n que puede cambiar el curso de una batalla."*
>
> **â€” El Manifiesto del Pokemon Battle AI**

## ğŸ“ Contacto

Para preguntas o colaboraciones:
- **Email**: <alex_gh@live.com>
- **LinkedIn**: [Alejandro Guerra Herrera](https://www.linkedin.com/in/alejandro-guerra-herrera-a86053115/)
- **GitHub**: [@AlexGHerrera](https://github.com/AlexGHerrera)

---

---

## ğŸŒŸ **Â¡Ãšnete a la Leyenda!**

â­ **Â¡Dale una estrella si este proyecto Ã©pico te ha inspirado!** â­

**Â¿Te atreves a crear un agente que aprenda a jugar Pokemon?**  
**Â¿LograrÃ¡s que supere a jugadores humanos?**  
**Â¿DescubrirÃ¡s estrategias que ni los maestros Pokemon conocen?**

### ğŸ”¥ **La aventura apenas comienza...**

*Fase 1 completada: Sabemos que los patrones existen.*  
*Fase 2 en progreso: Entendiendo las decisiones ganadoras.*  
*Fase 3+: Crear el agente que juegue y aprenda.*

**Â¡El camino hacia el Maestro Pokemon AI estÃ¡ trazado!** ğŸš€
